%!LW recipe=lualatex-shell-escape

\documentclass{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{listings}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{emoji}
\usetikzlibrary{positioning}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}

\makeatletter
\newcommand*{\indep}{%
  \mathbin{%
    \mathpalette{\@indep}{}%
  }%
}
\newcommand*{\nindep}{%
  \mathbin{%                   % The final symbol is a binary math operator
    \mathpalette{\@indep}{\not}% \mathpalette helps for the adaptation
                               % of the symbol to the different math styles.
  }%
}
\newcommand*{\@indep}[2]{%
  % #1: math style
  % #2: empty or \not
  \sbox0{$#1\perp\m@th$}%        box 0 contains \perp symbol
  \sbox2{$#1=$}%                 box 2 for the height of =
  \sbox4{$#1\vcenter{}$}%        box 4 for the height of the math axis
  \rlap{\copy0}%                 first \perp
  \dimen@=\dimexpr\ht2-\ht4-.2pt\relax
      % The equals symbol is centered around the math axis.
      % The following equations are used to calculate the
      % right shift of the second \perp:
      % [1] ht(equals) - ht(math_axis) = line_width + 0.5 gap
      % [2] right_shift(second_perp) = line_width + gap
      % The line width is approximated by the default line width of 0.4pt
  \kern\dimen@
  {#2}%
      % {\not} in case of \nindep;
      % the braces convert the relational symbol \not to an ordinary
      % math object without additional horizontal spacing.
  \kern\dimen@
  \copy0 %                       second \perp
} 

\DeclareMathOperator*{\argmin}{arg\,min}

\makeatother

% \usepackage[scale=2]{ccicons}
% \usepackage{epstopdf}
% \usepackage{xspace}
% \newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
% \usetheme{metropolis}           % use metropolis theme

\usepackage{changepage}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{textcomp}
\usepackage{color}
% \usepackage[dvipsnames]{xcolor}
\usepackage{pgf,tikz}
% \usepackage{url}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{array}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage[american]{babel}
\usepackage{enumerate}
\usepackage{minted}
\setminted[r]{fontsize=\scriptsize}
\setminted[python]{fontsize=\scriptsize}

% \renewcommand{\footnotesize}{\scriptsize}
% \usepackage[american]{babel}
% \usepackage[absolute,overlay]{textpos}

% % \title{test}
% \title[]{\texorpdfstring{Examining {\color{blue}SIENA} Model for the Estimation of
% {\color{red}Selection} and {\color{red}Influence}
% under {\color{blue}Misspecification}}}

\title[]{Experimentation Fundamentals \emoji{test-tube}}
\subtitle{Brief introduction and intuitions}

\author[shortname]{Sebastian Daza}
% \institute[shortinst]{
%     \inst{1} Department of Sociology, UW-Madison \\
% }

\date{}


\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline \emoji{world-map}}
\vspace{-0.1pt}
\begin{itemize}
    \item Introduction \& Causal Inference Fundamentals
    \item Experimental Design \& Randomization
    \item Power Analysis \& Sample Size
    \item Sampling \& Covariate Balance
    \item AB Analysis + Confounding \& Non-compliance Simulations
    % \item Benchmarks, Best Practices \& Common Pitfalls
    % \item Beyond AB Testing (DiD, SC, SDID, Geo-experiments)
\end{itemize}

\end{frame}

\subsection{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Python Package: \texttt{experiment-utils-pd} \emoji{package}}

A Python package for designing, analyzing, and validating experiments with advanced causal inference capabilities

\begin{center}
\includegraphics[width=0.7\textwidth]{figures/experiment-utils-package.png}
\end{center}

\vspace{0.3cm}
\textbf{Key components:}
\begin{itemize}
    \item \texttt{ExperimentAnalyzer} - AB test analysis, IV, IPW, regression adjustment, retrodesign
    \item \texttt{PowerSim} - Sample size \& power calculations, retrodesign
    % \item \texttt{utils} - Balanced random assignment
\end{itemize}

\vspace{0.1cm}
\textbf{Links:} \href{https://pypi.org/project/experiment-utils-pd}{PyPI} | \href{https://github.com/sdaza/experiment-utils-pd}{GitHub}

\end{frame}

\subsection{Causal Inference Fundamentals}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What problem are we trying to solve? \emoji{thinking-face}}

\textbf{Goal}: Measure the \red{causal effect} of a treatment/intervention

\vspace{0.3cm}
\textbf{\orange{The Fundamental Problem of Causal Inference:}}
\begin{itemize}
    \item For any individual, we can only observe \blue{one} potential outcome
    \vspace{0.1cm}
    \item Example: Did the medication work for patient $i$?
    \begin{itemize}
        \item We observe: Patient took medication $\rightarrow$ recovered in 5 days
        \item We \red{cannot} observe: Same patient without medication $\rightarrow$ ?
    \end{itemize}
    \vspace{0.1cm}
    \item \red{Individual causal effects are fundamentally unobservable}
    \vspace{0.2cm}
    \item We need a \blue{counterfactual}: What would have happened without treatment?
\end{itemize}

\vspace{0.2cm}
\textbf{Solution}: \red{Randomized experiments} (RCTs/A/B tests) create valid comparisons
\end{frame}


\begin{frame}{Why Randomization Works \emoji{game-die}}

\textbf{Without randomization:}
\begin{itemize}
    \item Treatment and control groups may differ in many ways
    \item Differences in outcomes could be due to pre-existing differences, not treatment
    \item Example: Sicker patients seek treatment $\rightarrow$ worse outcomes (confounding)
\end{itemize}

\vspace{0.3cm}
\textbf{With randomization:}
\begin{itemize}
    \item Treatment assignment is \blue{independent} of all other characteristics
    \item Groups are \orange{exchangeable}: same distribution of characteristics
    \item Any difference in outcomes can be attributed to treatment
\end{itemize}

% \vspace{0.2cm}
% \begin{block}{Key Insight}
% Randomization breaks the link between treatment and confounders
% \end{block}

\end{frame}


\begin{frame}{Key Assumptions for Valid Experiments \emoji{memo}}

\begin{enumerate}
    \item \textbf{\blue{Independence (Randomization)}}
    \begin{itemize}
        \item Random treatment assignment creates exchangeable groups
    \end{itemize}
    \vspace{0.2cm}
    
    \item \textbf{\orange{Stable Unit Treatment Value Assumption (SUTVA)}}
    \begin{itemize}
        \item \red{No interference}: Units don't affect each other
        \item \red{Consistency}: Treatment uniformly defined
        \item Violations: Network effects, marketplace spillovers
    \end{itemize}
    \vspace{0.2cm}
    
    \item \textbf{\red{Compliance}}
    \begin{itemize}
        \item Treatment received matches treatment assigned
    \end{itemize}
\end{enumerate}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Internal vs External Validity \emoji{dart}}

\textbf{\blue{Internal Validity}}: Can we trust the causal inference \red{within} our experiment?
\begin{itemize}
    \item \textbf{Threats:} Selection bias, implementation errors (SRM), measurement errors, SUTVA violations
\end{itemize}

\vspace{0.4cm}
\textbf{\orange{External Validity}}: Can we generalize \red{beyond} our experiment?
\begin{itemize}
    \item \textbf{Threats:} Non-representative samples, novelty effects, seasonal/context factors
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Experimental Designs: How to Assign Treatments? \emoji{test-tube}}

\textbf{\blue{Between-Subjects Design:}}
\begin{itemize}
    \item Each unit receives \blue{only one} treatment condition
    \item Compare \red{different} units: Group A vs Group B
    \item Example: 50\% users see version A, 50\% see version B
    \item \textbf{Key limitation:} More participants needed, lower power
\end{itemize}

\vspace{0.3cm}
\textbf{\orange{Within-Subjects Design:}}
\begin{itemize}
    \item Each unit receives \orange{all} treatment conditions (at different times)
    \item Compare \red{same} unit against itself
    \item Example: All users see version A in week 1, version B in week 2
    \item \textbf{Key limitation:} Order/carryover effects, not irreversible treatments
\end{itemize}
\end{frame}


\section{Power Analysis \& Sample Size}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Power \emoji{muscle}}

\textbf{Statistical Power} = Probability of detecting an effect when it exists

\vspace{0.3cm}
\textbf{Key components:}
\begin{itemize}
    \item \red{$\alpha$} (Type I error): False positive rate, typically 0.05
    \item \blue{$\beta$} (Type II error): False negative rate, typically 0.20
    \item \orange{Power} = $1 - \beta$, typically 0.80 (80\%)
    \item \textbf{Effect size}: Magnitude of difference
    \item \textbf{Sample size}: Number of units per group
\end{itemize}

For more details look at: \blue{\href{https://sdaza.com/blog/2025/statistical-power/}{blog's post on power analysis}}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Minimum Detectable Effect (MDE) \emoji{magnifying-glass-tilted-right}}

\textbf{MDE} = \blue{Smallest effect size} reliably detected at 80\% power, $\alpha$ = 0.05, given sample size

\vspace{0.2cm}
\textbf{\orange{How to define MDE:}}

\begin{enumerate}
    \item \textbf{Business:} Minimum effect for ROI (e.g., 2\% revenue lift)
    \item \textbf{Resource-constrained:} Achievable with sample (e.g., 50K users $\rightarrow$ 3\% MDE)
    \item \textbf{Historical:} Based on past experiments (typically 1-5\%)
\end{enumerate}

\vspace{0.2cm}
\begin{alertblock}{If MDE unreasonable \& limited sample:}
\vspace{0.2cm}
\begin{itemize}
    \item \red{Avoid running under-powered experiments}, risk of winner's curse (exaggerated estimates)
    \item Use quasi-experimental methods instead!
\end{itemize}
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Small Effects + Limited Traffic: What To Do? \emoji{light-bulb}}

\begin{enumerate}
    \item \textbf{\blue{Reduce variance}}: CUPED, regression adjustment, stratified randomization
    \item \textbf{\orange{Use sensitive metrics}}: surrogate/proxy metrics
    \item \textbf{\blue{Redesign experiment}}: within-subjects, responsive subpopulations, longer duration
    \item \textbf{\orange{Accumulate evidence}}: meta-analysis (\texttt{combine\_effects})
    \item \textbf{\red{Go quasi-experimental}}: DiD, Synthetic Control, Geo-experiments, Interrupted Time Series
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Multiple Comparisons Problem \emoji{fishing-pole}}

\textbf{Problem:} Testing $m$ metrics at $\alpha=0.05$ expects 1 false positive per 20 tests

$$P(\text{at least 1 false positive}) = 1 - (1-\alpha)^m$$

\vspace{0.3cm}
\textbf{Solutions:}
\begin{itemize}
    \item Multiple comparison corrections
    \item Bonferroni, Holm-Bonferroni, Sidak, Benjamini-Hochberg (FDR)
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Find Sample Size \& Power \emoji{technologist}}

\texttt{PowerSim} uses a simulation approach, more useful for more complex designs and metrics (compliance, multiple variants, etc.)

\vspace{1cm}
\begin{minted}[fontsize=\tiny]{python}
from experiment_utils.power_sim import PowerSim

p = PowerSim(metric='proportion',
            relative_effect=False,
            variants=2, 
            nsim=1000,
            alpha=0.05,
            alternative='two-tailed',
            comparisons=[(1, 0), (2, 0), (2, 1)],
            correction='holm')
\end{minted}

\end{frame}

\begin{frame}[fragile]{Find Sample Size \& Power \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
result = p.find_sample_size(
    target_power=0.80,
    baseline=0.10,
    effect=[0.03, 0.05],
    # compliance=0.80,
    optimize_allocation=True)

Using sample size 17202 (driven by (0, 1)) 
Optimized sample sizes: {'control': 1455, 'variant_1': 7873, 'variant_2': 7873}
Achieved power: {'(0, 1)': 0.907, '(0, 2)': 1.0, '(1, 2)': 0.942}

\end{minted}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Power Analysis in Practice \emoji{magnifying-glass-tilted-left}}

\vspace{0.2cm}
\begin{enumerate}

    \item \textbf{\blue{Always explore multiple scenarios}} --- not just one power calculation
    \begin{itemize}
        \item Jointly optimize: 80\% power + \red{realistic MDE} + acceptable duration
        \item In 1--2\% MDE territory: proxy metrics, CUPED
    \end{itemize}
    \vspace{0.15cm}

    \item \textbf{\blue{MDE is the most constrained parameter}} --- rarely inflated to compensate
    \begin{itemize}
        \item Should reflect \red{minimum business-relevant effect}, not statistical convenience
        \item If MDE is unrealistic, the experiment probably shouldn't run (quasi-exp instead)
    \end{itemize}
    \vspace{0.15cm}

    \item \textbf{\orange{Heuristics as smart defaults}}
    \begin{itemize}
        \item Keep power $\geq$ 80\% (most respected threshold)
        \item Cap MDE at 5\%; two-tailed tests preferred unless strong prior
    \end{itemize}

\end{enumerate}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Exploring the Power Surface \emoji{chart-increasing}}

\vspace{0.2cm}
\begin{minted}[fontsize=\tiny]{python}
p = PowerSim(metric='proportion', relative_effect=False,
             variants=1, nsim=1000)

rr = p.grid_sim_power(
    baseline_rates=0.25,
    effects=[0.005, 0.01, 0.02, 0.03, 0.04],
    sample_sizes=[1000, 2000, 3000, 5000, 8000, 10000],
    plot=True)
\end{minted}

\vspace{0.1cm}
\begin{center}
    \includegraphics[width=0.70\textwidth]{figures/grid_sim_power.png}
\end{center}

\end{frame}


\subsection{Sampling \& Randomization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Sampling Methods for Experiments \emoji{scissors}}

\textbf{Why sampling matters:}
\begin{itemize}
    \item Limited resources (budget, time, traffic)
    \item \blue{Reduces variance} - Better precision in estimates
    \item \red{Increases credibility} - Shows randomization worked properly
\end{itemize}

\vspace{0.2cm}
\textbf{Two main approaches:}
\begin{itemize}
    \item \blue{Random Sampling} - Simple random selection
    \begin{itemize}
        \item Easy to implement, may result in imbalanced small segments
    \end{itemize}
\vspace{0.1cm}
    \item \red{Stratified Sampling} - Sample within dimensions
    \begin{itemize}
        \item Ensures representation across strata, better for heterogeneous populations
        \item Can use proportional or equal allocation
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Blocking / Stratified Sampling \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
from experiment_utils.utils import balanced_random_assignment

# simulated data with covariates (3000 rows):
age  previous_purchases  days_since_signup  treatment  conversion
0  29.143694                   5         464.228095  treatment           0
1  49.973454                   2         577.496020    control           0
2  42.829785                   6         806.725758  treatment           1
3  24.937053                   6        1324.618460    control           0
4  34.213997                   5         415.036894  treatment           0

# random allocation
treatment = balanced_random_assignment(
    df,
    variants=['control', 'treatment'],
    allocation_ratio=0.5,
    check_balance_covariates=['age', 'previous_purchases', 'days_since_signup'],
    # balance_covariates=['age', 'previous_purchases', 'days_since_signup']
)
----------------------------------------------------------------------
         covariate  mean_treatment  mean_control       smd balanced
               age       40.046056     40.415916 -0.036537        y
previous_purchases        3.088000      3.004000  0.048583        y
 days_since_signup      365.495181    358.738388  0.018646        y

Summary: 3/3 covariates balanced (|SMD| < 0.1)
Mean |SMD|: 0.0346
Max |SMD|: 0.0486
======================================================================

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Blocking / Stratified Sampling \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}

# stratified allocation
treatment = balanced_random_assignment(
    df,
    variants=['control', 'treatment'],
    allocation_ratio=0.5,
    check_balance_covariates=['age', 'previous_purchases', 'days_since_signup'],
    balance_covariates=['age', 'previous_purchases', 'days_since_signup']
)

----------------------------------------------------------------------
         covariate  mean_control  mean_treatment       smd balanced
               age     40.167258       40.297051 -0.012818        y
previous_purchases      3.062868        3.028513  0.019869        y
 days_since_signup    362.615133      361.600167  0.002801        y

Summary: 3/3 covariates balanced (|SMD| < 0.1)
Mean |SMD|: 0.0118
Max |SMD|: 0.0199
======================================================================

\end{minted}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Regression with Pre-Treatment Covariates \emoji{robot}}

\textbf{Even with perfect randomization, regression adjustment helps:}

\vspace{0.2cm}
\begin{itemize}
    \item \blue{Reduced variance} $\rightarrow$ narrower confidence intervals, higher precision
    \item \orange{Increased power} to detect effects (especially with correlated covariates)
    \item \red{Corrects for chance imbalances} in small samples
\end{itemize}

\vspace{0.2cm}
\textbf{When to use:}
\begin{itemize}
    \item Pre-treatment covariates strongly correlated with outcome (most of the time)
    \item Sample size < 1000 per group (chance imbalances more likely)
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Regression Adjustment \emoji{robot}}

\begin{minted}[fontsize=\tiny]{python}
from experiment_utils.experiment_analyzer import ExperimentAnalyzer

# no adjustment
analyzer_simple = ExperimentAnalyzer(
    df,
    treatment_col='treatment',
    outcomes='conversion'
)
analyzer_simple.get_effects()
analyzer_simple.results['standard_error']
0.0173

# covariate adjustment
analyzer_adjusted = ExperimentAnalyzer(
    df,
    treatment_col='treatment',
    outcomes='conversion',
    covariates=['age', 'previous_purchases', 'days_since_signup'],
    regression_covariates=['age', 'previous_purchases', 'days_since_signup']
)

analyzer_adjusted.get_effects()
analyzer_adjusted.results['standard_error']
0.0122
\end{minted}


\end{frame}

% \begin{frame}{CUPED vs Regression Adjustment: Does It Matter? \emoji{disguised-face}}

% \vspace{0.1cm}
% \textbf{\orange{What actually matters!}}
% \begin{itemize}
%     \item Use \blue{pre-experiment $Y$} as covariate $\rightarrow$ typically highest $\rho$, easy to automate
%     \item Include \red{treatment $\times$ covariate interactions} (per-group slopes)
%     \begin{itemize}
%         \item Without: pooled $\theta$ $\rightarrow$ might \red{inflate} variance with unequal splits
%         \item With: \blue{asymptotically always $\geq$ as efficient} as the unadjusted estimator
%     \end{itemize}
%     \item Use robust standard errors
% \end{itemize}

% % \vspace{0.2cm}
% % \textbf{Bottom line:} variance reduction $\approx \rho^2_{XY}$, so \blue{covariate choice} matters more than the label. Pre-experiment outcome is usually best; adding more covariates helps if they explain residual variance (\href{https://building.nubank.com/3-lessons-from-implementing-controlled-experiment-using-pre-experiment-data-cuped-at-nubank/}{Nubank lessons})
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Winner's Curse \emoji{skull}}

\textbf{Problem:} Significant results often \red{overestimate} true effect

\vspace{0.2cm}
\textbf{Why?}
\begin{itemize}
    \item Selection bias: Only "winners" reported
    \item Small samples + low power $\rightarrow$ worse exaggeration (2-3x for power < 0.50)
\end{itemize}
\vspace{0.2cm}
$$\text{Exaggeration (Type M)} = \left|\frac{\hat{\tau}}{\tau}\right|$$
\vspace{-0.2cm}
$$\text{Relative Bias} = \frac{\hat{\tau}}{\tau}$$

\vspace{0.1cm}
{\small When an underpowered experiment reports a significant effect, divide the estimate by the exaggeration ratio to get a more realistic sense of the true effect size.}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Winner's Curse \emoji{skull}}

\begin{minted}[fontsize=\tiny]{python}

# reduce the sample size (~500 per group)
sdf = df.sample(1000)
analyzer_simple = ExperimentAnalyzer(
    sdf,
    treatment_col='treatment',
    outcomes='conversion'
)
analyzer_simple.get_effects()
analyzer_simple.results[['absolute_effect', 'pvalue']]
   absolute_effect    pvalue
0         0.076136  0.011805

# let's assess the potential bias (we know the MDE is 0.045)
analyzer_simple.calculate_retrodesign(true_effect=0.045)
cols = ['power', 'type_s_error', 'type_m_error', 'relative_bias', 'trimmed_abs_effect']
print(analyzer_simple.calculate_retrodesign(true_effect=true_effect)[cols])

   power  type_s_error  type_m_error  relative_bias  trimmed_abs_effect
0  0.336        0.0006        1.7265         1.7248              0.04414
\end{minted}

\end{frame}


\section{More on analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Components \emoji{straight-ruler}}

\textbf{Key components:}

\vspace{0.3cm}
\begin{itemize}
    \item \blue{Primary metrics}
    \begin{itemize}
        \item Pre-specified metrics (e.g., revenue, conversion)
        \item Multiple tests?
    \end{itemize}
    \vspace{0.2cm}
    \item \orange{Guardrail metrics}
    \begin{itemize}
        \item Safety checks (e.g., load time, error rates)
        \item Implementation metrics
    \end{itemize}
    \vspace{0.2cm}
    \item \red{Sample Ratio Mismatch (SRM)}
    \begin{itemize}
        \item Does split match expectation?
        \item SRM signals implementation issues or bias
    \end{itemize}
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Simple analysis \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
from experiment_utils import ExperimentAnalyzer

analyzer = ExperimentAnalyzer(
    df,
    treatment_col='treatment',
    outcomes='conversion',
    bootstrap=True,
    exp_sample_ratio_col='expected_sample_ratio',
    outcome_models={'conversion':['ols', 'logistic']},
    # pvalue_adjustment='sidak',
)
analyzer.get_effects()
print(analyzer.results.round(3)[['model_type', 'absolute_effect', 'relative_effect', 
    'rel_effect_lower', 'rel_effect_upper', 'srm_pvalue']])

  model_type  absolute_effect  relative_effect  rel_effect_lower  rel_effect_upper  srm_pvalue
0        ols            0.056            0.179             0.075             0.309       0.324
1   logistic            0.056            0.179             0.074             0.307       0.324
\end{minted}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Distribution Assumptions: Don't Overthink It \emoji{brain}}

% \vspace{0.3cm}
\textbf{Why not useful:}
\begin{itemize}
    \item \blue{CLT:} Sample means $\approx$ normal for large n (>30-50), regardless of distribution
    \item Normality tests too sensitive; \orange{sampling distribution} matters, not population
\end{itemize}

\vspace{0.3cm}
\textbf{What to do:}
\begin{itemize}
    \item \textbf{Large samples:} Use standard t-tests / z-tests (rely on CLT)
    \item \textbf{Small samples/complex metrics:} Use \blue{bootstrapping}
    \item \textbf{Extreme outliers:} Robust statistics (better models!)
\end{itemize}

\end{frame}

\subsection{Simulation: Non-Compliance + Confounding}

\begin{frame}{What If the Experiment Is Broken? \emoji{scream}}

\textbf{\red{Common problems:}}
\begin{itemize}
    \item \textbf{Implementation issues} - Bugs in assignment or logging
    \item \textbf{SRM} - Observed split differs from expected
    \item \textbf{Non-compliance} - Users don't receive assigned treatment
\end{itemize}

\vspace{0.2cm}
\textbf{\blue{What can we do?}}
\begin{itemize}
    \item \textbf{ITT} - Preserves randomization, underestimates effect
    \item \textbf{Regression adjustment / IPW} - Correct for imbalances
    \item \textbf{IV} - Assignment as instrument (ITT $\rightarrow$ LATE)
\end{itemize}

\end{frame}


\begin{frame}{Simulation: Non-Compliance + Confounding \emoji{technologist}}

\textbf{CS farming meeting} scenario:
\begin{itemize}
    \item Only a \% of treated users \blue{attend} (one-sided non-compliance)
    \item Attenders are more engaged, higher baseline revenue (\red{confounding})
\end{itemize}

\vspace{0.2cm}
\textbf{How to recover the true effect on bookings?}
\vspace{0.1cm}

\begin{description}
    \item[\blue{ITT}] As assigned $\rightarrow$ \red{underestimates} effect
    \item[\blue{Regression}] Covariate adjustment $\rightarrow$ reduces bias
    \item[\blue{IPW}] Inverse probability weighting $\rightarrow$ corrects imbalance
    \item[\blue{IV}] Assignment as instrument $\rightarrow$ \orange{LATE} $\equiv$ \blue{ATT} (one-sided non-compliance)
\end{description}

\end{frame}


\begin{frame}[fragile]{Naive Approach \emoji{teddy-bear}}

\textbf{Naive analysis:} Compare attenders vs non-attenders

\textbf{True effect} = 5 bookings

\begin{minted}[fontsize=\tiny]{python}

naive = ExperimentAnalyzer(
    cdf,
    treatment_col='attended',
    outcomes='bookings',
)

naive.get_effects()

naive.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']]

absolute_effect  abs_effect_lower  abs_effect_upper         pvalue
0         6.316905          5.921429          6.712381  3.823925e-215

\end{minted}
\end{frame}


\begin{frame}[fragile]{Intend-to-Treat (ITT) \emoji{balance-scale}}

\textbf{ITT:} Compare all treated vs control, regardless of attendance

\textbf{True effect} = 5 bookings

\begin{minted}[fontsize=\tiny]{python}

itt = ExperimentAnalyzer(
    cdf,
    treatment_col='assigned',
    outcomes='bookings',
)
itt.get_effects()

itt.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']]

absolute_effect  abs_effect_lower  abs_effect_upper        pvalue
0         2.743512          2.359612          3.127413  1.418363e-44

\end{minted}
\end{frame}


\begin{frame}[fragile]{Inverse Probability Weighting (IPW) \emoji{ninja}}

\textbf{IPW:} Adjust for compliance selection using covariates

\textbf{True effect} = 5 bookings
% \textbf{Note:} We are estimating ATT (effect on attenders) ~ effect on compliers

\begin{minted}[fontsize=\tiny]{python}

ipw = ExperimentAnalyzer(
    cdf,
    treatment_col='attended', outcomes='bookings',
    covariates=['engagement', 'account_age_months', 'monthly_usage'],
    adjustment='balance',  balance_method='ps-logistic', target_effect='ATT', overlap_plot=True)

ipw.get_effects()
ipw.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']]
   absolute_effect  abs_effect_lower  abs_effect_upper         pvalue
0         5.056775          4.652374          5.461177  1.212354e-132

# what about just regression adjustment?
reg_adj = ExperimentAnalyzer(
    cdf,
    treatment_col='attended', outcomes='bookings',
    covariates=['engagement', 'account_age_months', 'monthly_usage'],
    regression_covariates=['engagement', 'account_age_months', 'monthly_usage'])

reg_adj.get_effects()
reg_adj.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper']]
   absolute_effect  abs_effect_lower  abs_effect_upper         pvalue
0         5.086338          4.766525          5.406151  2.597419e-213

\end{minted}
\end{frame}


\begin{frame}{Overlap Assumption \emoji{magnifying-glass-tilted-left}}

\textbf{\red{Why it matters:}}
\begin{itemize}
    \item IPW requires \blue{common support}: every unit needs a non-zero probability of either treatment
    \item Without it, weights become extreme $\rightarrow$ \red{high variance and bias}
\end{itemize}

\vspace{0.1cm}
\textbf{\blue{What to do:}}
\begin{itemize}
    \item \textbf{Diagnose:} Inspect propensity score distributions by group
    \item \textbf{Trim:} Drop or down-weight units outside common support
    \item \textbf{Target:} Shift estimand from ATE $\rightarrow$ ATT
\end{itemize}

\vspace{0.1cm}
\begin{center}
    \includegraphics[width=0.30\textwidth]{figures/ipw-overlap.png}
\end{center}

\end{frame}


\begin{frame}[fragile]{Instrumental Variables (IV) \emoji{key}}

\textbf{IV:} Assignment as instrument $\rightarrow$ One-sided non-compliance $\rightarrow$ LATE $\equiv$ ATT (compliers = attenders)

\small{Since always-takers don't exist, every attender is a complier.}

\textbf{True effect} = 5 bookings


\begin{minted}[fontsize=\tiny]{python}

iv = ExperimentAnalyzer(
    cdf,
    treatment_col='attended',
    outcomes='bookings',
    covariates=['engagement', 'account_age_months', 'monthly_usage'],
    instrument_col='assigned',
    adjustment='IV'
)
iv.get_effects()
print(iv.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']])
   absolute_effect  abs_effect_lower  abs_effect_upper  pvalue
0         4.870595          4.381111           5.36008     0.0

\end{minted}

\small{We only want to use the part of attendance variation that came from the random assignment. I'm throwing away self-selection.}

\end{frame}


\begin{frame}{Fixed-Effects Meta-Analysis \emoji{bar-chart}}

\textbf{Goal:} Pool effect estimates across multiple experiments into a single estimate

\vspace{0.2cm}
\textbf{\blue{Inverse-variance weighting:}}
\begin{itemize}
    \item Each experiment's estimate is weighted by $w_i = 1/\sigma_i^2$
    \item More precise experiments (smaller SE) get \blue{more weight}
    % \item Pooled estimate: $\hat{\tau} = \frac{\sum_i w_i \hat{\tau}_i}{\sum_i w_i}$, \quad $\text{SE}(\hat{\tau}) = 1/\sqrt{\sum_i w_i}$
\end{itemize}

\vspace{0.2cm}
\textbf{\orange{When to use it:}}
\begin{itemize}
    \item Multiple experiments testing the \blue{same intervention}
    \item Any single experiment underpowered on its own
    \item Experiments run across different \red{regions / segments}
\end{itemize}

\vspace{0.1cm}
\textbf{\red{Key assumption:}} All experiments share the same true effect (homogeneity)

\end{frame}


\begin{frame}[fragile]{Fixed-Effects Meta-Analysis \emoji{technologist}}

\textbf{Setup:} 5 experiments, \red{baselines correlated with allocation} $\rightarrow$ naive pooling is biased

\begin{minted}[fontsize=\tiny]{python}
# high-alloc -> low baseline; low-alloc -> high baseline
experiments = [
    {"name": "exp_1", "n": 3000, "alloc": 0.20, "baseline": 0.20},
    {"name": "exp_2", "n": 2000, "alloc": 0.30, "baseline": 0.25},
    {"name": "exp_3", "n": 1500, "alloc": 0.50, "baseline": 0.30},
    {"name": "exp_4", "n": 1000, "alloc": 0.70, "baseline": 0.40},
    {"name": "exp_5", "n": 800,  "alloc": 0.80, "baseline": 0.50},
]
# true_effect = 0.05 in all experiments

# naive pooled analysis: ignores experiment structure
naive = ExperimentAnalyzer(data=meta_df, treatment_col="treatment", outcomes=["conversion"])
naive.get_effects()
# absolute_effect = 0.1397   <-- 2.8x the true effect!

# correct: fixed-effects meta-analysis
analyzer = ExperimentAnalyzer(
    data=meta_df, treatment_col="treatment",
    outcomes=["conversion"], experiment_identifier="experiment",
)
analyzer.get_effects()
pooled = analyzer.combine_effects(grouping_cols=["outcome"])
\end{minted}

\end{frame}


\begin{frame}[fragile]{Fixed-Effects Meta-Analysis \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
# per-experiment results
print(analyzer.results[["experiment", "absolute_effect", "standard_error", "pvalue"]].round(4))
  experiment  absolute_effect  standard_error  pvalue
      exp_1           0.0758          0.0200  0.0001
      exp_2           0.0319          0.0221  0.1494
      exp_3           0.0333          0.0247  0.1774
      exp_4           0.0800          0.0342  0.0193
      exp_5           0.0203          0.0443  0.6464

print(pooled[["outcome", "experiments", "absolute_effect", "standard_error", "pvalue"]])
      outcome  experiments  absolute_effect  standard_error    pvalue
   conversion          5.0           0.0514          0.0115  0.000008

# True effect = 0.05
# Naive pooled  = 0.14  <-- 2.8x overestimate
# Meta-analysis = 0.05  <-- recovers true effect
\end{minted}
\end{frame}


\begin{frame}{Key takeaways \emoji{sparkles}}

\begin{itemize}
    \item Power analysis is crucial for experiment design; MDE should reflect business relevance, not statistical convenience
    \item Sampling methods (random vs stratified) can improve balance and precision
    \item Regression adjustment reduces variance and corrects for imbalances, even with perfect randomization
    \item Non-compliance and confounding can be addressed with ITT, IPW, and IV methods
    \item Meta-analysis allows pooling across multiple experiments
\end{itemize}

\end{frame}


\begin{frame}{Links}

\begin{itemize}
    \item \href{https://github.com/sdaza/slides/blob/main/presentations/experimentation/code/simulations.py}{Simulation code}
    \item \texttt{\href{https://github.com/sdaza/experiment-utils-pd}{experiment-utils-pd}}
\end{itemize}

\end{frame}

\end{document}

