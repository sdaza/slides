%!LW recipe=lualatex-shell-escape

\documentclass{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{listings}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{emoji}
\usetikzlibrary{positioning}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}

\makeatletter
\newcommand*{\indep}{%
  \mathbin{%
    \mathpalette{\@indep}{}%
  }%
}
\newcommand*{\nindep}{%
  \mathbin{%                   % The final symbol is a binary math operator
    \mathpalette{\@indep}{\not}% \mathpalette helps for the adaptation
                               % of the symbol to the different math styles.
  }%
}
\newcommand*{\@indep}[2]{%
  % #1: math style
  % #2: empty or \not
  \sbox0{$#1\perp\m@th$}%        box 0 contains \perp symbol
  \sbox2{$#1=$}%                 box 2 for the height of =
  \sbox4{$#1\vcenter{}$}%        box 4 for the height of the math axis
  \rlap{\copy0}%                 first \perp
  \dimen@=\dimexpr\ht2-\ht4-.2pt\relax
      % The equals symbol is centered around the math axis.
      % The following equations are used to calculate the
      % right shift of the second \perp:
      % [1] ht(equals) - ht(math_axis) = line_width + 0.5 gap
      % [2] right_shift(second_perp) = line_width + gap
      % The line width is approximated by the default line width of 0.4pt
  \kern\dimen@
  {#2}%
      % {\not} in case of \nindep;
      % the braces convert the relational symbol \not to an ordinary
      % math object without additional horizontal spacing.
  \kern\dimen@
  \copy0 %                       second \perp
} 

\DeclareMathOperator*{\argmin}{arg\,min}

\makeatother

% \usepackage[scale=2]{ccicons}
% \usepackage{epstopdf}
% \usepackage{xspace}
% \newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
% \usetheme{metropolis}           % use metropolis theme

\usepackage{changepage}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{textcomp}
\usepackage{color}
% \usepackage[dvipsnames]{xcolor}
\usepackage{pgf,tikz}
% \usepackage{url}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{array}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage[american]{babel}
\usepackage{enumerate}
\usepackage{minted}
\setminted[r]{fontsize=\scriptsize}
\setminted[python]{fontsize=\scriptsize}

% \renewcommand{\footnotesize}{\scriptsize}
% \usepackage[american]{babel}
% \usepackage[absolute,overlay]{textpos}

% % \title{test}
% \title[]{\texorpdfstring{Examining {\color{blue}SIENA} Model for the Estimation of
% {\color{red}Selection} and {\color{red}Influence}
% under {\color{blue}Misspecification}}}

\title[]{Experimentation Fundamentals \emoji{test-tube}}
\subtitle{Brief introduction and intuitions}

\author[shortname]{Sebastian Daza}
% \institute[shortinst]{
%     \inst{1} Department of Sociology, UW-Madison \\
% }

\date{}


\begin{document}
\maketitle

\AtBeginSection{
  \begin{frame}[plain,noframenumbering]
    \sectionpage
  \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline \emoji{world-map}}
\vspace{-0.1pt}
\begin{itemize}
    \item Causal Inference Fundamentals
    \item Experimental Design \& Randomization
    \item Power Analysis \& Sample Size
    \item Sampling \& Covariate Balance
    \item Analysis
    \item Non-Compliance \& Confounding
    % \item Benchmarks, Best Practices \& Common Pitfalls
    % \item Beyond AB Testing (DiD, SC, SDID, Geo-experiments)
\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Python Package: \texttt{experiment-utils-pd} \emoji{package}}

\begin{center}
\includegraphics[width=0.65\textwidth]{figures/experiment-utils-package.png}
\end{center}

\textbf{Key components:}
\begin{itemize}
    \item \texttt{ExperimentAnalyzer} - AB test analysis, IV, IPW, regression adjustment, retrodesign
    \item \texttt{PowerSim} - Sample size \& power calculations, retrodesign
    \item \texttt{utils} - Balanced random assignment
\end{itemize}

\textbf{Links:} \href{https://pypi.org/project/experiment-utils-pd}{PyPI} | \href{https://github.com/sdaza/experiment-utils-pd}{GitHub}

\end{frame}


\section{Causal Inference Fundamentals}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{What problem are we trying to solve? \emoji{thinking-face}}

\textbf{Goal}: Measure the \red{causal effect} of a treatment/intervention

\vspace{0.1cm}
\textbf{\orange{The Fundamental Problem of Causal Inference:}}
\begin{itemize}
    \item For any individual, we can only observe \blue{one} potential outcome
    \item Example: Did the medication work for patient $i$?
    \begin{itemize}
        \item We observe: Patient took medication $\rightarrow$ recovered in 5 days
        \item We \red{cannot} observe: Same patient without medication $\rightarrow$ ?
    \end{itemize}
    \item \red{Individual causal effects are fundamentally unobservable}
    \item We need a \blue{counterfactual}: What would have happened without treatment?
\end{itemize}

\vspace{0.1cm}
\textbf{Solution}: \red{Randomized experiments} (RCTs/A/B tests) create valid comparisons
\end{frame}


\begin{frame}{Why Randomization Works \emoji{game-die}}

\textbf{Without randomization:}
\begin{itemize}
    \item Treatment and control groups may differ in many ways
    \item Differences in outcomes could be due to pre-existing differences, not treatment
    \item Example: Sicker patients seek treatment $\rightarrow$ worse outcomes (confounding)
\end{itemize}

\vspace{0.1cm}
\textbf{With randomization:}
\begin{itemize}
    \item Treatment assignment is \blue{independent} of all other characteristics
    \item Groups are \orange{exchangeable}: same distribution of characteristics
    \item Any difference in outcomes can be attributed to treatment
\end{itemize}

\end{frame}


\begin{frame}{Key Assumptions for Valid Experiments \emoji{memo}}

\begin{enumerate}
    \item \textbf{\blue{Independence (Randomization)}}
    \begin{itemize}
        \item Random treatment assignment creates exchangeable groups
    \end{itemize}

    \item \textbf{\orange{Stable Unit Treatment Value Assumption (SUTVA)}}
    \begin{itemize}
        \item \red{No interference}: Units don't affect each other
        \item \red{Consistency}: Treatment uniformly defined
        \item Violations: Network effects, marketplace spillovers
    \end{itemize}

    \item \textbf{\red{Compliance}}
    \begin{itemize}
        \item Treatment received matches treatment assigned
    \end{itemize}
\end{enumerate}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Internal vs External Validity \emoji{dart}}

\textbf{\blue{Internal Validity}}: Can we trust the causal inference \red{within} our experiment?
\begin{itemize}
    \item \textbf{Threats:} Selection bias, implementation errors (SRM), measurement errors, SUTVA violations
\end{itemize}

\vspace{0.15cm}
\textbf{\orange{External Validity}}: Can we generalize \red{beyond} our experiment?
\begin{itemize}
    \item \textbf{Threats:} Non-representative samples, novelty effects, seasonal/context factors
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Experimental Designs: How to Assign Treatments? \emoji{test-tube}}

\textbf{\blue{Between-Subjects Design:}}
\begin{itemize}
    \item Each unit receives \blue{only one} condition; compare \red{different} units (A vs B)
    \item \textit{E.g.,} 50\% see version A, 50\% see version B
    \item \textbf{Limitation:} More participants needed, lower power
\end{itemize}
\textbf{\orange{Within-Subjects Design:}}
\begin{itemize}
    \item Each unit receives \orange{all} conditions at different times; compare \red{same} unit
    \item \textit{E.g.,} All users see A in week 1, B in week 2
    \item \textbf{Limitation:} Order/carryover effects; not for irreversible treatments
\end{itemize}
\end{frame}

\begin{frame}{Estimand, Estimator, Estimate \emoji{dart}}

\begin{description}[leftmargin=0pt]
    \item[\blue{Estimand}] \textbf{What} we want to know --- the target quantity
    \begin{itemize}
        \item \textit{E.g.,} ATE: $\mathbb{E}[Y(1) - Y(0)]$; also ATT (on the treated), LATE (on compliers)
    \end{itemize}

    \item[\orange{Estimator}] \textbf{How} we compute it --- the method or formula
    \begin{itemize}
        \item \textit{E.g.,} difference in means, OLS, IPW --- same estimand can have multiple estimators
    \end{itemize}

    \item[\red{Estimate}] \textbf{What we get} --- the actual number from our data
    \begin{itemize}
        \item \textit{E.g.,} $\hat{\tau} = +0.056$ (5.6 pp lift); always comes with a standard error
    \end{itemize}
\end{description}

\end{frame}

\section{Power Analysis \& Sample Size}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Power \emoji{muscle}}

\textbf{Statistical Power} = Probability of detecting an effect when it exists

\vspace{0.3cm}
\textbf{Key components:}
\begin{itemize}
    \item \red{$\alpha$} (Type I error): False positive rate, typically 0.05
    \item \blue{$\beta$} (Type II error): False negative rate, typically 0.20
    \item \orange{Power} = $1 - \beta$, typically 0.80 (80\%)
    \item \textbf{Effect size}: Magnitude of difference
    \item \textbf{Sample size}: Number of units per group
\end{itemize}

For more details look at: \blue{\href{https://sdaza.com/blog/2025/statistical-power/}{blog's post on power analysis}}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Minimum Detectable Effect (MDE) \emoji{magnifying-glass-tilted-right}}

\textbf{MDE} = \blue{Smallest effect size} reliably detected at 80\% power, $\alpha$ = 0.05, given sample size

\vspace{0.2cm}
\textbf{\orange{How to define MDE:}}

\begin{enumerate}
    \item \textbf{Business:} Minimum effect for ROI (e.g., 2\% revenue lift)
    \item \textbf{Resource-constrained:} Achievable with sample (e.g., 50K users $\rightarrow$ 3\% MDE)
    \item \textbf{Historical:} Based on past experiments (typically 1-5\%)
\end{enumerate}

\vspace{0.2cm}
\begin{alertblock}{If MDE unreasonable \& limited sample:}
\vspace{0.2cm}
\begin{itemize}
    \item \red{Avoid running under-powered experiments}, risk of winner's curse (exaggerated estimates)
    \item Use quasi-experimental methods instead!
\end{itemize}
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Small Effects + Limited Traffic: What To Do? \emoji{light-bulb}}

\begin{enumerate}
    \item \textbf{\blue{Reduce variance}}: CUPED (pre-experiment covariate adjustment), regression adjustment (control for pre-treatment covariates), stratified randomization
    \item \textbf{\orange{Use sensitive metrics}}: surrogate/proxy metrics
    \item \textbf{\blue{Redesign experiment}}: within-subjects, responsive subpopulations, longer duration
    \item \textbf{\orange{Accumulate evidence}}: meta-analysis
    \item \textbf{\red{Go quasi-experimental}}: DiD (Difference-in-Differences), Synthetic Control, Geo-experiments, Interrupted Time Series
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Multiple Comparisons Problem \emoji{fishing-pole}}

\textbf{Problem:} Every test has a 5\% false positive rate by chance. Testing $m$ metrics simultaneously inflates this:

$$P(\text{at least 1 false positive}) = 1 - (1-\alpha)^m \quad \xrightarrow{m=10} \quad \approx 40\%$$

\textbf{Two correction strategies:}
\begin{description}
    \item[\blue{FWER}] Control probability of \emph{any} false positive\\
        Bonferroni, Holm-Bonferroni --- stricter, fewer discoveries
    \item[\orange{FDR}] Control the \emph{proportion} of false positives among significant results\\
        Benjamini-Hochberg --- less strict, better when testing many metrics
\end{description}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Find Sample Size \& Power \emoji{technologist}}

\texttt{PowerSim} uses a simulation approach, more useful for more complex designs and metrics (compliance, multiple variants, etc.)

\vspace{1cm}
\begin{minted}[fontsize=\tiny]{python}
from experiment_utils import PowerSim

p = PowerSim(metric='proportion',
            relative_effect=False,
            variants=2, 
            nsim=1000,
            alpha=0.05,
            alternative='two-tailed',
            comparisons=[(1, 0), (2, 0), (2, 1)],
            correction='holm')
\end{minted}

\end{frame}

\begin{frame}[fragile]{Find Sample Size \& Power \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
result = p.find_sample_size(
    target_power=0.80,
    baseline=0.10,
    effect=[0.03, 0.05],
    # compliance=0.80,
    optimize_allocation=True)

Using sample size 12926 (driven by (0, 1)) 
Optimized sample sizes: {'control': 1093, 'variant_1': 5916, 'variant_2': 5916}
Achieved power: {'(0, 1)': 0.839, '(0, 2)': 0.986, '(1, 2)': 0.873}

\end{minted}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Power Analysis in Practice \emoji{magnifying-glass-tilted-left}}

\begin{enumerate}

    \item \textbf{\blue{Always explore multiple scenarios}} --- not just one power calculation
    \begin{itemize}
        \item Jointly optimize: 80\% power + \red{realistic MDE} + acceptable duration
        \item In 1--2\% MDE territory: proxy metrics, CUPED
    \end{itemize}

    \item \textbf{\blue{MDE is the most constrained parameter}} --- rarely inflated to compensate
    \begin{itemize}
        \item Should reflect \red{minimum business-relevant effect}, not statistical convenience
        \item If MDE is unrealistic, the experiment probably shouldn't run (quasi-exp instead)
    \end{itemize}

    \item \textbf{\orange{Heuristics as smart defaults}}
    \begin{itemize}
        \item Keep power $\geq$ 80\% (most respected threshold)
        \item Cap MDE at 5\%; two-tailed tests preferred unless strong prior
    \end{itemize}

\end{enumerate}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Exploring the Power Surface \emoji{chart-increasing}}

\begin{minted}[fontsize=\tiny]{python}
p = PowerSim(metric='proportion', 
            relative_effect=False, 
            variants=1,
            nsim=1000)

rr = p.grid_sim_power(baseline_rates=0.25,
                effects=[0.005, 0.01, 0.02, 0.03, 0.04],
                sample_sizes=[1000, 2000, 3000, 5000, 8000, 10000],
                hue='effect',
                threads=16,
                plot=True)
\end{minted}

\vspace{0.1cm}
\begin{center}
    \includegraphics[width=0.55\textwidth]{figures/grid_sim_power.png}
\end{center}

\end{frame}


\section{Sampling \& Covariate Balance}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Sampling Methods for Experiments \emoji{scissors}}

\textbf{Why sampling matters:}
\begin{itemize}
    \item Limited resources (budget, time, traffic)
    \item \blue{Reduces variance} - Better precision in estimates
    \item \red{Increases credibility} - Shows randomization worked properly
\end{itemize}

\textbf{Two main approaches:}
\begin{itemize}
    \item \blue{Random Sampling} - Simple random selection
    \begin{itemize}
        \item Easy to implement, may result in imbalanced small segments
    \end{itemize}
    \item \red{Stratified Sampling} - Sample within dimensions
    \begin{itemize}
        \item Ensures representation across strata, better for heterogeneous populations
        \item Can use proportional or equal allocation
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Blocking / Stratified Sampling \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
from experiment_utils import balanced_random_assignment

# random allocation
treatment_unblock = balanced_random_assignment(
    df,
    variants=['treatment', 'control'],
    allocation_ratio=1/2,
    balance_covariates=['age', 'previous_purchases', 'days_since_signup'],
    seed=4321
)

Balance Check After Assignment

Comparison: control (n=1,500) vs treatment (n=1,500)

         covariate  n_control  n_treatment  mean_control  mean_treatment      smd balanced
               age       1500         1500     39.629457       39.578661 0.005167        y
previous_purchases       1500         1500      3.072000        2.958000 0.065706        y 
 days_since_signup       1500         1500    367.491830      345.775354 0.060344        y

Summary: 3/3 covariates balanced (|SMD| < 0.1)
Mean |SMD|: 0.0437
Max |SMD|: 0.0657

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Blocking / Stratified Sampling \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}

# stratified allocation
treatment_block = balanced_random_assignment(
    df,
    variants=['treatment', 'control'],
    allocation_ratio=1/2,
    stratification_covariates=['age', 'previous_purchases', 'days_since_signup'],
    seed=4321
)

Balance Check After Assignment

Comparison: treatment (n=1,500) vs control (n=1,500)
         covariate  n_treatment  n_control  mean_treatment  mean_control      smd balanced
               age         1500       1500       39.656748     39.551370 0.010718        y
previous_purchases         1500       1500        3.015333      3.014667 0.000384        y
 days_since_signup         1500       1500      358.103009    355.164175 0.008163        y

Summary: 3/3 covariates balanced (|SMD| < 0.1)
Mean |SMD|: 0.0064
Max |SMD|: 0.0107

\end{minted}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Distribution Assumptions: Don't Overthink It \emoji{brain}}

% \vspace{0.3cm}
\textbf{Why not useful:}
\begin{itemize}
    \item \blue{CLT (Central Limit Theorem):} Sample means $\approx$ normal for large n (>30-50), regardless of distribution
    \item Normality tests too sensitive; \orange{sampling distribution} matters, not population
\end{itemize}

\vspace{0.1cm}
\textbf{What to do:}
\begin{itemize}
    \item \textbf{Large samples:} Use standard t-tests / z-tests (rely on CLT)
    \item \textbf{Small samples/complex metrics:} Use \blue{bootstrapping}
    \item \textbf{Extreme outliers:} Robust statistics (better models!)
\end{itemize}

\end{frame}

\section{Analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Components \emoji{straight-ruler}}

\textbf{Key components:}
\begin{itemize}
    \item \blue{Primary metrics}
    \begin{itemize}
        \item Pre-specified metrics (e.g., revenue, conversion)
        \item Multiple tests?
    \end{itemize}
    \item \orange{Guardrail metrics}
    \begin{itemize}
        \item Safety checks (e.g., load time, error rates)
        \item Implementation metrics
    \end{itemize}
    \item \red{Sample Ratio Mismatch (SRM)}
    \begin{itemize}
        \item Does split match expectation?
        \item SRM signals implementation issues or bias
    \end{itemize}
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Simple analysis \emoji{technologist} (1,500 users/group, 5 pp lift)}

\begin{minted}[fontsize=\tiny]{python}
from experiment_utils import ExperimentAnalyzer

analyzer_simple = ExperimentAnalyzer(
    df,
    treatment_col='treatment',
    outcomes='conversion',
    bootstrap=True,
    exp_sample_ratio=0.50,
    outcome_models={'conversion':['ols', 'logistic']},
    # pvalue_adjustment='sidak',
)
analyzer_simple.get_effects()
print(analyzer_simple.results.round(3)[['model_type', 'absolute_effect', 'relative_effect', 'srm_pvalue']])

  model_type  absolute_effect  relative_effect  srm_pvalue
0        ols            0.051            0.153         1.0
1   logistic            0.051            0.153         1.0
\end{minted}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Regression with Pre-Treatment Covariates \emoji{robot}}

\textbf{Regression adjustment} controls for pre-treatment covariates to partial out noise unrelated to treatment.

\textbf{Benefits (even with perfect randomization):}
\begin{itemize}
    \item \blue{Reduced variance} $\rightarrow$ narrower confidence intervals, higher precision
    \item \orange{Increased power} (especially when covariates correlate with outcome)
    \item \red{Corrects chance imbalances} in small samples ($n < 1000$/group)
    \item \textbf{Use when:} pre-treatment covariates available --- almost always worth it
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Regression Adjustment \emoji{robot}}

\begin{minted}[fontsize=\tiny]{python}
from experiment_utils import ExperimentAnalyzer

# no adjustment
analyzer_simple = ExperimentAnalyzer(
    df,
    treatment_col='treatment',
    outcomes='conversion',

)
analyzer_simple.get_effects()
print(analyzer_simple.results[['absolute_effect', 'pvalue', 'standard_error']])

   absolute_effect    pvalue  standard_error
0         0.051333  0.003411        0.017531

# covariate adjustment
analyzer_adjusted = ExperimentAnalyzer(
    df,
    treatment_col='treatment',
    outcomes='conversion',
    regression_covariates=['age', 'previous_purchases', 'days_since_signup']
)

analyzer_adjusted.get_effects()
print(analyzer_adjusted.results[['absolute_effect', 'pvalue', 'standard_error']])
   absolute_effect    pvalue  standard_error
0         0.050259  0.000039        0.012212

final_effect = analyzer_adjusted.results.loc[0, 'absolute_effect']

\end{minted}


\end{frame}

% \begin{frame}{CUPED vs Regression Adjustment: Does It Matter? \emoji{disguised-face}}

% \vspace{0.1cm}
% \textbf{\orange{What actually matters!}}
% \begin{itemize}
%     \item Use \blue{pre-experiment $Y$} as covariate $\rightarrow$ typically highest $\rho$, easy to automate
%     \item Include \red{treatment $\times$ covariate interactions} (per-group slopes)
%     \begin{itemize}
%         \item Without: pooled $\theta$ $\rightarrow$ might \red{inflate} variance with unequal splits
%         \item With: \blue{asymptotically always $\geq$ as efficient} as the unadjusted estimator
%     \end{itemize}
%     \item Use robust standard errors
% \end{itemize}

% % \vspace{0.2cm}
% % \textbf{Bottom line:} variance reduction $\approx \rho^2_{XY}$, so \blue{covariate choice} matters more than the label. Pre-experiment outcome is usually best; adding more covariates helps if they explain residual variance (\href{https://building.nubank.com/3-lessons-from-implementing-controlled-experiment-using-pre-experiment-data-cuped-at-nubank/}{Nubank lessons})
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Winner's Curse \emoji{skull} (Subsample of 500 users/group)}

\textbf{Problem:} Significant results often \red{overestimate} true effect

\vspace{0.2cm}
\textbf{Why?}
\begin{itemize}
    \item Selection bias: Only "winners" reported
    \item Small samples + low power $\rightarrow$ worse exaggeration (2-3x for power < 0.50)
\end{itemize}
\vspace{0.2cm}
$$\text{Exaggeration (Type M)} = \left|\frac{\hat{\tau}}{\tau}\right|$$
\vspace{-0.2cm}
$$\text{Relative Bias} = \frac{\hat{\tau}}{\tau}$$

\vspace{0.1cm}
{\small When an underpowered experiment reports a significant effect, divide the estimate by the exaggeration ratio to get a more realistic sense of the true effect size.}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Winner's Curse \emoji{skull}}

\begin{minted}[fontsize=\tiny]{python}

# we hunted for significant effect using a smaller sample (n=500/group)
print(analyzer_retro.results[['absolute_effect', 'pvalue', 'standard_error']])
   absolute_effect    pvalue  standard_error
0         0.080322  0.007778        0.030179

print(f'True effect: {final_effect:.4f}')
True effect: 0.0503

cols = ['power', 'type_s_error', 'type_m_error', 'relative_bias', 'trimmed_abs_effect']
print(analyzer_retro.calculate_retrodesign(true_effect=final_effect)[cols])
    power  type_s_error  type_m_error  relative_bias  trimmed_abs_effect
0  0.3996           0.0        1.5829         1.5829            0.050743

\end{minted}

\end{frame}

\section{Non-Compliance \& Confounding}

\begin{frame}{What If the Experiment Is Broken? \emoji{scream}}

\textbf{\red{Common problems:}}
\begin{itemize}
    \item \textbf{Implementation issues} - Bugs in assignment or logging
    \item \textbf{SRM (Sample Ratio Mismatch)} - Observed split differs from expected
    \item \textbf{Non-compliance} - Users don't receive assigned treatment
\end{itemize}

\vspace{0.2cm}
\textbf{\blue{What can we do?}}
\begin{itemize}
    \item \textbf{ITT (Intent to Treat)} - Preserves randomization, underestimates effect
    \item \textbf{Regression adjustment / IPW (Inverse Probability Weighting)} - Correct for imbalances
    \item \textbf{IV (Instrumental Variables)} - Assignment as instrument (ITT $\rightarrow$ LATE)
\end{itemize}

\end{frame}


\begin{frame}{Simulation: Non-Compliance + Confounding \emoji{technologist}}

\textbf{CS farming meeting} scenario:
\begin{itemize}
    \item Only a \% of treated users \blue{attend} (one-sided non-compliance)
    \item Attenders are more engaged, higher baseline revenue (\red{confounding})
\end{itemize}

\textbf{How to recover the true effect on bookings?}

\begin{description}
    \item[\blue{ITT}] As assigned $\rightarrow$ \red{underestimates} effect
    \item[\blue{Regression}] Covariate adjustment $\rightarrow$ reduces bias
    \item[\blue{IPW}] Inverse probability weighting $\rightarrow$ corrects imbalance
    \item[\blue{IV}] Assignment as instrument $\rightarrow$ \orange{LATE} $\equiv$ \blue{ATT} (one-sided non-compliance)
\end{description}

\end{frame}


\begin{frame}[fragile]{Naive Approach \emoji{teddy-bear}}

\textbf{Naive analysis:} Compare attenders vs non-attenders

\textbf{True effect} = 5 bookings

\begin{minted}[fontsize=\tiny]{python}

naive = ExperimentAnalyzer(
    cdf,
    treatment_col='attended',
    outcomes='bookings',
)

naive.get_effects()

naive.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']]

absolute_effect  abs_effect_lower  abs_effect_upper         pvalue
0         6.316905          5.921429          6.712381  3.823925e-215

\end{minted}
\end{frame}


\begin{frame}[fragile]{Intent-to-Treat (ITT) \emoji{balance-scale}}

\textbf{ITT:} Compare all treated vs control, regardless of attendance

\textbf{True effect} = 5 bookings

\begin{minted}[fontsize=\tiny]{python}

itt = ExperimentAnalyzer(
    cdf,
    treatment_col='assigned',
    outcomes='bookings',
)
itt.get_effects()

itt.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']]

absolute_effect  abs_effect_lower  abs_effect_upper        pvalue
0         2.743512          2.359612          3.127413  1.418363e-44

\end{minted}
\end{frame}


\begin{frame}[fragile]{Inverse Probability Weighting (IPW) \emoji{ninja}}

\textbf{IPW:} Adjust for compliance selection using covariates

\textbf{True effect} = 5 bookings
% \textbf{Note:} We are estimating ATT (effect on attenders) ~ effect on compliers

\begin{minted}[fontsize=\tiny]{python}

ipw = ExperimentAnalyzer(
    cdf,
    treatment_col='attended', outcomes='bookings',
    balance_covariates=['engagement', 'account_age_months', 'monthly_usage'],
    adjustment='balance',  balance_method='ps-logistic', estimand='ATT', overlap_plot=True)

ipw.get_effects()
ipw.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']]
   absolute_effect  abs_effect_lower  abs_effect_upper         pvalue
0         5.056775          4.652374          5.461177  1.212354e-132

\end{minted}
\end{frame}


\begin{frame}{Overlap Assumption \emoji{magnifying-glass-tilted-left}}

\textbf{\red{Why it matters:}}
\begin{itemize}
    \item IPW requires \blue{common support}: every unit needs a non-zero probability of either treatment
    \item Without it, weights become extreme $\rightarrow$ \red{high variance and bias}
\end{itemize}

\textbf{\blue{What to do:}}
\begin{itemize}
    \item \textbf{Diagnose:} Inspect propensity score distributions by group
    \item \textbf{Trim:} Drop or down-weight units outside common support
    \item \textbf{Target:} Shift estimand from ATE $\rightarrow$ ATT
\end{itemize}

\end{frame}


\begin{frame}[fragile]{Instrumental Variables (IV) \emoji{key}}

\textbf{IV:} Use assignment as instrument to isolate the effect on compliers (attenders)\\
\small{One-sided non-compliance: LATE $\equiv$ ATT, since there are no always-takers.}

\textbf{True effect} = 5 bookings

\begin{minted}[fontsize=\tiny]{python}
iv = ExperimentAnalyzer(
    cdf,
    treatment_col='attended',
    outcomes='bookings',
    regression_covariates=['engagement', 'account_age_months', 'monthly_usage'],
    instrument_col='assigned',
    adjustment='IV'
)
iv.get_effects()
print(iv.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']])
   absolute_effect  abs_effect_lower  abs_effect_upper  pvalue
0         4.870595          4.381111           5.36008     0.0

\end{minted}

\end{frame}


\begin{frame}{Fixed-Effects Meta-Analysis \emoji{bar-chart}}

\textbf{Goal:} Pool effect estimates across multiple experiments into a single estimate

\textbf{\blue{Inverse-variance weighting:}}
\begin{itemize}
    \item Each experiment's estimate is weighted by $w_i = 1/\sigma_i^2$
    \item More precise experiments (smaller SE) get \blue{more weight}
\end{itemize}

\textbf{\orange{When to use it:}}
\begin{itemize}
    \item Multiple experiments testing the \blue{same intervention}
    \item Any single experiment underpowered on its own
    \item Experiments run across different \red{regions / segments}
\end{itemize}

\textbf{\red{Key assumption:}} All experiments share the same true effect (homogeneity)

\end{frame}


\begin{frame}[fragile]{Fixed-Effects Meta-Analysis \emoji{technologist}}

\textbf{Setup:} 5 experiments, \red{baselines correlated with allocation} $\rightarrow$ naive pooling is biased

\begin{minted}[fontsize=\tiny]{python}
# high-alloc -> low baseline; low-alloc -> high baseline
experiments = [
    {"name": "exp_1", "n": 3000, "alloc": 0.20, "baseline": 0.20},
    {"name": "exp_2", "n": 2000, "alloc": 0.30, "baseline": 0.25},
    {"name": "exp_3", "n": 1500, "alloc": 0.50, "baseline": 0.30},
    {"name": "exp_4", "n": 1000, "alloc": 0.70, "baseline": 0.40},
    {"name": "exp_5", "n": 800,  "alloc": 0.80, "baseline": 0.50},
]
# true_effect = 0.05 in all experiments

# naive pooled analysis: ignores experiment structure
naive = ExperimentAnalyzer(data=meta_df, treatment_col="treatment", outcomes=["conversion"])
naive.get_effects()
# absolute_effect = 0.1431   <-- 2.9x the true effect!

# correct: fixed-effects meta-analysis
analyzer = ExperimentAnalyzer(
    data=meta_df, treatment_col="treatment",
    outcomes=["conversion"], experiment_identifier="experiment",
)
analyzer.get_effects()
pooled = analyzer.combine_effects(grouping_cols=["outcome"])
\end{minted}

\end{frame}


\begin{frame}[fragile]{Fixed-Effects Meta-Analysis \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
# per-experiment results
print(analyzer.results[["experiment", "absolute_effect", "standard_error", "pvalue"]].round(4))
  experiment  absolute_effect  standard_error  pvalue
      exp_1           0.0558          0.0196  0.0043
      exp_2           0.0621          0.0225  0.0057
      exp_3           0.0707          0.0244  0.0038
      exp_4           0.0586          0.0341  0.0856
      exp_5           0.0406          0.0443  0.3596

print(pooled[["outcome", "experiments", "absolute_effect", "standard_error", "pvalue"]])
      outcome  experiments  absolute_effect  standard_error    pvalue
   conversion          5.0           0.0600          0.0114  1.54e-07

# True effect = 0.05
# Naive pooled  = 0.14  <-- 2.9x overestimate
# Meta-analysis = 0.06  <-- recovers true effect
\end{minted}
\end{frame}


\begin{frame}{Key takeaways \emoji{sparkles}}

\begin{itemize}
    \item Power analysis is crucial for experiment design; MDE should reflect business relevance, not statistical convenience
    \item Sampling methods (random vs stratified) can improve balance and precision
    \item Regression adjustment reduces variance and corrects for imbalances, even with perfect randomization
    \item Non-compliance and confounding can be addressed with ITT, IPW, and IV methods
    \item Meta-analysis allows pooling across multiple experiments
\end{itemize}

\end{frame}


\begin{frame}{Next Steps \& Beyond \emoji{rocket}}

\textbf{\blue{The Inference Cycle}}

\begin{center}
\scalebox{0.70}{%
\begin{tikzpicture}[node distance=0.8cm, every node/.style={align=center, font=\tiny}]
    \node[draw, rounded corners, fill=blue!10] (design) {\textbf{Design}\\randomization,\\power, sampling};
    \node[draw, rounded corners, fill=orange!10, right=of design] (estimation) {\textbf{Estimation}\\analysis,\\causal inference};
    \node[draw, rounded corners, fill=red!10, right=of estimation] (optimization) {\textbf{Optimization}\\uplift modeling,\\personalization};
    \draw[->, thick] (design) -- (estimation);
    \draw[->, thick] (estimation) -- (optimization);
    \draw[->, thick, dashed, bend left=25] (optimization) to node[above]{\tiny inform next experiment} (design);
\end{tikzpicture}}
\end{center}

\vspace{0.1cm}
\textbf{\orange{Other techniques to explore:}}
\begin{itemize}
    \item \textbf{Quasi-experimental:} DiD, Synthetic Control, Geo-experiments, ITS
    \item \textbf{Heterogeneous effects:} Uplift modeling, causal forests (HTE)
    \item \textbf{Sequential testing:} Always-valid inference, alpha-spending (peeking)
    \item \textbf{Bayesian experimentation:} Prior incorporation, posterior decision rules
\end{itemize}

\end{frame}


\begin{frame}{Links}

\begin{itemize}
    \item \href{https://github.com/sdaza/slides/blob/main/presentations/experimentation/code/simulations.py}{Simulation code}
    \item \texttt{\href{https://github.com/sdaza/experiment-utils-pd}{experiment-utils-pd}}
\end{itemize}

\end{frame}

\end{document}

