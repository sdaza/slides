%!LW recipe=lualatex-shell-escape

\documentclass{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{listings}
\usepackage{pifont}
\usepackage{tikz}
\usepackage{emoji}
\usetikzlibrary{positioning}
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} 

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}

\makeatletter
\newcommand*{\indep}{%
  \mathbin{%
    \mathpalette{\@indep}{}%
  }%
}
\newcommand*{\nindep}{%
  \mathbin{%                   % The final symbol is a binary math operator
    \mathpalette{\@indep}{\not}% \mathpalette helps for the adaptation
                               % of the symbol to the different math styles.
  }%
}
\newcommand*{\@indep}[2]{%
  % #1: math style
  % #2: empty or \not
  \sbox0{$#1\perp\m@th$}%        box 0 contains \perp symbol
  \sbox2{$#1=$}%                 box 2 for the height of =
  \sbox4{$#1\vcenter{}$}%        box 4 for the height of the math axis
  \rlap{\copy0}%                 first \perp
  \dimen@=\dimexpr\ht2-\ht4-.2pt\relax
      % The equals symbol is centered around the math axis.
      % The following equations are used to calculate the
      % right shift of the second \perp:
      % [1] ht(equals) - ht(math_axis) = line_width + 0.5 gap
      % [2] right_shift(second_perp) = line_width + gap
      % The line width is approximated by the default line width of 0.4pt
  \kern\dimen@
  {#2}%
      % {\not} in case of \nindep;
      % the braces convert the relational symbol \not to an ordinary
      % math object without additional horizontal spacing.
  \kern\dimen@
  \copy0 %                       second \perp
} 

\DeclareMathOperator*{\argmin}{arg\,min}

\makeatother

% \usepackage[scale=2]{ccicons}
% \usepackage{epstopdf}
% \usepackage{xspace}
% \newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
% \usetheme{metropolis}           % use metropolis theme

\usepackage{changepage}
\usepackage{threeparttable}
\usepackage{tabularx}
\usepackage{textcomp}
\usepackage{color}
% \usepackage[dvipsnames]{xcolor}
\usepackage{pgf,tikz}
% \usepackage{url}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{dcolumn}
\usepackage{array}
\usepackage{siunitx}
\usepackage{multirow}
\usepackage[american]{babel}
\usepackage{enumerate}
\usepackage{minted}
\setminted[r]{fontsize=\scriptsize}
\setminted[python]{fontsize=\scriptsize}

% \renewcommand{\footnotesize}{\scriptsize}
% \usepackage[american]{babel}
% \usepackage[absolute,overlay]{textpos}

% % \title{test}
% \title[]{\texorpdfstring{Examining {\color{blue}SIENA} Model for the Estimation of
% {\color{red}Selection} and {\color{red}Influence}
% under {\color{blue}Misspecification}}}

\title[]{Experimentation Fundamentals \emoji{test-tube}}
\subtitle{Brief introduction and intuitions}

\author[shortname]{Sebastian Daza}
% \institute[shortinst]{
%     \inst{1} Department of Sociology, UW-Madison \\
% }

\date{}


\begin{document}
\maketitle

\AtBeginSection{
  \begin{frame}[plain,noframenumbering]
    \sectionpage
  \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}{Outline \emoji{world-map}}
\vspace{-0.1pt}
\begin{itemize}
    \item Causal Inference Fundamentals
    \item Experimental Design
    \begin{itemize}
        \item Power Analysis \& Sample Size
        \item Sampling \& Stratification
    \end{itemize}
    \item Analysis
    \begin{itemize}
        \item Non-Compliance \& Confounding
    \end{itemize}
    % \item Benchmarks, Best Practices \& Common Pitfalls
    % \item Beyond AB Testing (DiD, SC, SDID, Geo-experiments)
\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Python Package: \texttt{experiment-utils-pd} \emoji{package}}

\begin{center}
\includegraphics[width=0.65\textwidth]{figures/experiment-utils-package.png}
\end{center}

\textbf{Key components:}
\begin{itemize}
    \item \texttt{ExperimentAnalyzer} - AB test analysis, IV, IPW, regression adjustment, retrodesign
    \item \texttt{PowerSim} - Sample size \& power calculations, retrodesign
    \item \texttt{utils} - Balanced random assignment
\end{itemize}

\textbf{Links:} \href{https://pypi.org/project/experiment-utils-pd}{PyPI} | \href{https://github.com/sdaza/experiment-utils-pd}{GitHub}

\end{frame}


\section{Causal Inference Fundamentals}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The Ladder of Causation (Pearl) \emoji{bar-chart}}

\renewcommand{\arraystretch}{1.35}
\begin{tabularx}{\textwidth}{clXl}
\toprule
\textbf{Rung} & \textbf{Level} & \textbf{Question} & \textbf{DS Role} \\
\midrule
1 & \blue{Association} \textit{(Owl)}         & \textit{``What is?''}             & Prediction       \\
2 & \orange{Intervention} \textit{(Baby)}     & \textit{``What if I do\ldots?''}  & Decisions        \\
3 & \red{Counterfactual} \textit{(Scientist)} & \textit{``What if I had\ldots?''} & ITE / CATE       \\
\bottomrule
\end{tabularx}

\vspace{0.1cm}
{\footnotesize
\begin{itemize}\setlength{\itemsep}{0.02cm}
    \item \blue{Rung 1}: $P(Y \mid X)$ --- patterns in observed data; home of predictive ML
    \item \orange{Rung 2}: $P(Y \mid do(X))$ --- effect of actions; needs experiments or assumptions
    \item \red{Rung 3}: $P(Y_x \mid x', y')$ --- individual counterfactuals; structural causal models
\end{itemize}
}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Rung 1 --- The Owl: Seeing Patterns \emoji{owl}}

$P(Y \mid X)$ --- \textit{``Given I observe X, what is Y likely to be?''}

\vspace{0.2cm}
\begin{itemize}\setlength{\itemsep}{0.12cm}
    \item \textbf{Industry home:} risk prediction, churn, fraud detection, recommendations
    \item \textbf{Task:} find patterns in data $\rightarrow$ guess an outcome
    \item \textbf{Powerful for:} monitoring, alerting, targeting, ranking
    \item \red{\textbf{Cannot tell you:} what to \textit{change} to get a different outcome}
\end{itemize}

\vspace{0.2cm}
{\small A churn model tells you \blue{who} will churn --- not \red{what intervention would prevent it}}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The Data Validity Cliff \emoji{warning}}

The name captures the idea: your data has \blue{solid validity} for prediction --- then you \red{step off a cliff} the moment you use it to justify an intervention.

\vspace{0.08cm}
{\small The cliff is confounding: $P(Y \mid X) \neq P(Y \mid do(X))$ --- your model learned \emph{who} tends to have high $Y$, not \emph{what causes} $Y$ to change}

\vspace{0.12cm}
\textbf{Classic fall: Selection vs.\ Policy}
\begin{itemize}\setlength{\itemsep}{0.04cm}
    \item \textbf{Observation:} \textit{``Users who adopt Feature X have 40\% higher engagement''}
    \item \textbf{Decision:} Ship Feature X to everyone $\rightarrow$ engagement will rise
    \item \textbf{Reality:} Engaged users \blue{self-selected} into Feature X --- the engagement was already there
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Rung 2 --- The Baby: Doing \& Deciding \emoji{test-tube}}

$P(Y \mid do(X))$ --- \textit{``What happens if we intervene?''} \quad \orange{\textbf{Business decisions live here}}

\vspace{0.15cm}
\begin{itemize}\setlength{\itemsep}{0.08cm}
    \item \orange{\textbf{A/B test}}: randomization manufactures the $do$ --- cleanest path to Rung 2
    \item \textbf{Not always possible:} ethical constraints, time horizons, cost, irreversibility
\end{itemize}

\vspace{0.15cm}
\textbf{When you can't experiment $\rightarrow$ Identification:}
\begin{itemize}\setlength{\itemsep}{0.05cm}
    \item Build a bridge: $P(Y \mid X) \rightarrow P(Y \mid do(X))$ using assumptions
    \item Tools: DiD, IV, regression discontinuity, synthetic control
    \item \red{\textbf{The data can never give you the assumptions}} --- they come from theory
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Rung 3 --- The Scientist: Imagining \emoji{microscope}}

\textbf{A/B tests $\rightarrow$ ATE}: average effect across the population \\
{\footnotesize Nothing specific about individuals or subgroups}

\vspace{0.06cm}
\textbf{Rung 3 asks:} \textit{``What would have happened to \emph{this} unit had we acted differently?''} \\
{\footnotesize Requires a \orange{structural causal model} --- logic + domain knowledge, not just data}

\vspace{0.06cm}
\textbf{Email campaign example:}
\begin{itemize}\setlength{\itemsep}{0.02cm}
    \item {\small Rung 2: ``Does the campaign increase purchases on average?'' $\rightarrow$ holdout test $\rightarrow$ ATE}
    \item {\small Rung 3: ``Would \emph{this} customer have bought without the email?'' $\rightarrow$ purchase intent model}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{Putting It Together: Doctor Onboarding \(\rightarrow\) Bookings \emoji{clipboard}}

% \begin{itemize}\setlength{\itemsep}{0.15cm}
%     \item \blue{\textbf{Rung 1 (Owl):}} \textit{``Doctors who complete onboarding have 35\% more bookings''} \\
%           {\footnotesize Selection bias: motivated doctors do both --- $P(\text{bookings} \mid \text{onboarding})$}

%     \item \orange{\textbf{Rung 2 (Baby):}} \textit{``The new flow caused a $+6\%$ lift in bookings''} \\
%           {\footnotesize A/B test or DiD $\Rightarrow$ ATE --- $P(\text{bookings} \mid do(\text{onboarding}))$}

%     \item \red{\textbf{Rung 3 (Scientist):}} \textit{``Would Dr.~\#47291 have reached 5 bookings/week without it?''} \\
%           {\footnotesize ITE / uplift model --- strong structural assumptions required}
% \end{itemize}

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{How Stakeholders Talk --- and the Tough Conversation \emoji{speech-balloon}}

% \begin{itemize}\setlength{\itemsep}{0.1cm}

%     \item \blue{\textbf{Rung 1}} {\footnotesize\red{(sounds causal, but isn't)}}
%     \begin{itemize}\setlength{\itemsep}{0.01cm}
%         \item[] \small\textit{``Onboarding \textbf{drives} bookings'' / ``X is a key \textbf{lever} for retention''}
%         \item[] \small\blue{These are associations --- push back when stakeholders claim causality}
%     \end{itemize}

%     \item \orange{\textbf{Rung 2}} {\footnotesize(valid only after a controlled experiment)}
%     \begin{itemize}\setlength{\itemsep}{0.01cm}
%         \item[] \small\textit{``The new flow \textbf{caused} a $+6\%$ \textbf{lift}'' / ``measure the \textbf{impact}''}
%     \end{itemize}

%     \item \red{\textbf{Rung 3}}
%     \begin{itemize}\setlength{\itemsep}{0.01cm}
%         \item[] \small\textit{``Would they have churned \textbf{anyway}?'' / ``How much was \textbf{incremental}?''}
%         \item[] \small\textit{``Does our mobile result apply to desktop?''} {\footnotesize--- transportability}
%         \item[] \small\orange{Needs theory + structural model, not just more data}
%     \end{itemize}

% \end{itemize}

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{frame}{What is the Role of Uplift Modeling? \emoji{chart-increasing}}

% \begin{center}
% A/B test $\rightarrow$ \blue{\textbf{ATE}}: does the treatment work \emph{on average}? \\[0.04cm]
% Uplift model $\rightarrow$ \orange{\textbf{CATE}}: for \emph{whom} does it work?
% \end{center}

% \vspace{0.04cm}
% \textbf{Still Rung 2 fundamentally} --- needs A/B data to train \\
% {\small Gets closer to Rung 3 thinking: estimating heterogeneous effects}

% \vspace{0.08cm}
% \textbf{Target the ``persuadables'':}
% \begin{itemize}\setlength{\itemsep}{0.02cm}
%     \item \blue{Always-takers}: convert regardless $\rightarrow$ no incremental value
%     \item \red{Never-takers}: won't convert $\rightarrow$ wasted resources
%     \item \orange{Persuadables}: respond to treatment $\rightarrow$ \textbf{target these}
% \end{itemize}

% \vspace{0.04cm}
% {\small \red{Caveat:} Individual counterfactual still unobservable --- strong assumptions required for validation}

% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{The Fundamental Problem (Rung 2) \emoji{test-tube}}

For any unit, we only observe \blue{one} potential outcome --- never both

\vspace{0.1cm}
\begin{itemize}\setlength{\itemsep}{0.05cm}
    \item Dr.~\#47291 got the new onboarding flow $\rightarrow$ 8 bookings/week
    \item We \red{cannot} observe: same doctor with the old flow $\rightarrow$ \textbf{?}
    \item \red{Individual causal effects are unobservable} (Rung 3 territory)
\end{itemize}

\vspace{0.1cm}
\textbf{Rung 2 solution}: compare \blue{groups} $\rightarrow$ estimate the \blue{ATE}

\vspace{0.05cm}
\begin{itemize}\setlength{\itemsep}{0.05cm}
    \item \orange{A/B test}: random assignment creates a counterfactual group
    \item \blue{Quasi-experiments} (DiD, IV, RD): when randomization is not possible
\end{itemize}

\end{frame}



\begin{frame}{Key Assumptions for Valid Experiments \emoji{memo}}

\begin{enumerate}
    \item \textbf{\blue{Independence (Randomization)}}
    \begin{itemize}
        \item Random treatment assignment creates exchangeable groups
    \end{itemize}

    \item \textbf{\orange{Stable Unit Treatment Value Assumption (SUTVA)}}
    \begin{itemize}
        \item \red{No interference}: Units don't affect each other
        \item \red{Consistency}: Treatment uniformly defined
        \item Violations: Network effects, marketplace spillovers
    \end{itemize}

    \item \textbf{\red{Compliance}}
    \begin{itemize}
        \item Treatment received matches treatment assigned
    \end{itemize}
\end{enumerate}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Internal vs External Validity \emoji{dart}}

\textbf{\blue{Internal Validity}}: Can we trust the causal inference \red{within} our experiment?
\begin{itemize}
    \item \textbf{Threats:} Selection bias, implementation errors (SRM), measurement errors, SUTVA violations
\end{itemize}

\vspace{0.15cm}
\textbf{\orange{External Validity}}: Can we generalize \red{beyond} our experiment?
\begin{itemize}
    \item \textbf{Threats:} Non-representative samples, novelty effects, seasonal/context factors
\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Experimental Designs: How to Assign Treatments? \emoji{test-tube}}

\textbf{\blue{Between-Subjects Design:}}
\begin{itemize}
    \item Each unit receives \blue{only one} condition; compare \red{different} units (A vs B)
    \item \textit{E.g.,} 50\% see version A, 50\% see version B
    \item \textbf{Limitation:} More participants needed, lower power
\end{itemize}
\textbf{\orange{Within-Subjects Design:}}
\begin{itemize}
    \item Each unit receives \orange{all} conditions at different times; compare \red{same} unit
    \item \textit{E.g.,} All users see A in week 1, B in week 2
    \item \textbf{Limitation:} Order/carryover effects; not for irreversible treatments
\end{itemize}
\end{frame}

\begin{frame}{Estimand, Estimator, Estimate \emoji{dart}}

\begin{description}[leftmargin=0pt]
    \item[\blue{Estimand}] \textbf{What} we want to know --- the target quantity
    \begin{itemize}
        \item \textit{E.g.,} ATE: $\mathbb{E}[Y(1) - Y(0)]$; also ATT (on the treated), LATE (on compliers)
    \end{itemize}

    \item[\orange{Estimator}] \textbf{How} we compute it --- the method or formula
    \begin{itemize}
        \item \textit{E.g.,} difference in means, OLS, IPW --- same estimand can have multiple estimators
    \end{itemize}

    \item[\red{Estimate}] \textbf{What we get} --- the actual number from our data
    \begin{itemize}
        \item \textit{E.g.,} $\hat{\tau} = +0.056$ (5.6 pp lift); always comes with a standard error
    \end{itemize}
\end{description}

\end{frame}

\section{Power Analysis \& Sample Size}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Power \emoji{muscle}}

\textbf{Statistical Power} = Probability of detecting an effect when it exists

\vspace{0.3cm}
\textbf{Key components:}
\begin{itemize}
    \item \red{$\alpha$} (Type I error): False positive rate, typically 0.05
    \item \blue{$\beta$} (Type II error): False negative rate, typically 0.20
    \item \orange{Power} = $1 - \beta$, typically 0.80 (80\%)
    \item \textbf{Effect size}: Magnitude of difference
    \item \textbf{Sample size}: Number of units per group
\end{itemize}

For more details look at: \blue{\href{https://sdaza.com/blog/2025/statistical-power/}{blog's post on power analysis}}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Minimum Detectable Effect (MDE) \emoji{magnifying-glass-tilted-right}}

\textbf{MDE} = \blue{Smallest effect size} reliably detected at 80\% power, $\alpha$ = 0.05, given sample size

\vspace{0.2cm}
\textbf{\orange{How to define MDE:}}

\begin{enumerate}
    \item \textbf{Business:} Minimum effect for ROI (e.g., 2\% revenue lift)
    \item \textbf{Resource-constrained:} Achievable with sample (e.g., 50K users $\rightarrow$ 3\% MDE)
    \item \textbf{Historical:} Based on past experiments (typically 1-5\%)
\end{enumerate}

\vspace{0.2cm}
\begin{alertblock}{If MDE unreasonable \& limited sample:}
\vspace{0.2cm}
\begin{itemize}
    \item \red{Avoid running under-powered experiments}, risk of winner's curse (exaggerated estimates)
    \item Use quasi-experimental methods instead!
\end{itemize}
\end{alertblock}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Small Effects + Limited Traffic: What To Do? \emoji{light-bulb}}

\begin{enumerate}
    \item \textbf{\blue{Reduce variance}}: CUPED (pre-experiment covariate adjustment), regression adjustment (control for pre-treatment covariates), stratified randomization
    \item \textbf{\orange{Use sensitive metrics}}: surrogate/proxy metrics
    \item \textbf{\blue{Redesign experiment}}: within-subjects, responsive subpopulations, longer duration
    \item \textbf{\orange{Accumulate evidence}}: meta-analysis
    \item \textbf{\red{Go quasi-experimental}}: DiD (Difference-in-Differences), Synthetic Control, Geo-experiments, Interrupted Time Series
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Power Analysis in Practice \emoji{magnifying-glass-tilted-left}}

\begin{enumerate}

    \item \textbf{\blue{Always explore multiple scenarios}} --- not just one power calculation
    \begin{itemize}
        \item Jointly optimize: 80\% power + \red{realistic MDE} + acceptable duration
        \item In 1--2\% MDE territory: proxy metrics, CUPED
    \end{itemize}

    \item \textbf{\blue{MDE is the most constrained parameter}} --- rarely inflated to compensate
    \begin{itemize}
        \item Should reflect \red{minimum business-relevant effect}, not statistical convenience
        \item If MDE is unrealistic, the experiment probably shouldn't run (quasi-exp instead)
    \end{itemize}

    \item \textbf{\orange{Heuristics as smart defaults}}
    \begin{itemize}
        \item Keep power $\geq$ 80\% (most respected threshold)
        \item Cap MDE at 5\%; two-tailed tests preferred unless strong prior
    \end{itemize}

\end{enumerate}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Multiple Comparisons Problem \emoji{fishing-pole}}

\textbf{Problem:} Every test has a 5\% false positive rate by chance. Testing $m$ metrics simultaneously inflates this:

$$P(\text{at least 1 false positive}) = 1 - (1-\alpha)^m \quad \xrightarrow{m=10} \quad \approx 40\%$$

\textbf{Two correction strategies:}
\begin{description}
    \item[\blue{FWER}] Control probability of \emph{any} false positive\\
        Bonferroni, Holm-Bonferroni --- stricter, fewer discoveries
    \item[\orange{FDR}] Control the \emph{proportion} of false positives among significant results\\
        Benjamini-Hochberg --- less strict, better when testing many metrics
\end{description}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[fragile]{Find Sample Size \& Power \emoji{technologist}}

\texttt{PowerSim} uses a simulation approach, more useful for more complex designs and metrics (compliance, multiple variants, etc.)

\vspace{1cm}
\begin{minted}[fontsize=\tiny]{python}
from experiment_utils import PowerSim

p = PowerSim(metric='proportion',
            relative_effect=False,
            variants=2, 
            nsim=1000,
            alpha=0.05,
            alternative='two-tailed',
            comparisons=[(1, 0), (2, 0), (2, 1)],
            correction='holm')
\end{minted}

\end{frame}

\begin{frame}[fragile]{Find Sample Size \& Power \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
result = p.find_sample_size(
    target_power=0.80,
    baseline=0.10,
    effect=[0.03, 0.05],
    # compliance=0.80,
    optimize_allocation=True)

Using sample size 12926 (driven by (0, 1)) 
Optimized sample sizes: {'control': 1093, 'variant_1': 5916, 'variant_2': 5916}
Achieved power: {'(0, 1)': 0.839, '(0, 2)': 0.986, '(1, 2)': 0.873}

\end{minted}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Exploring the Power Surface \emoji{chart-increasing}}

\begin{minted}[fontsize=\tiny]{python}
p = PowerSim(metric='proportion', 
            relative_effect=False, 
            variants=1,
            nsim=1000)

rr = p.grid_sim_power(baseline_rates=0.25,
                effects=[0.005, 0.01, 0.02, 0.03, 0.04],
                sample_sizes=[1000, 2000, 3000, 5000, 8000, 10000],
                hue='effect',
                threads=16,
                plot=True)
\end{minted}

\vspace{0.1cm}
\begin{center}
    \includegraphics[width=0.55\textwidth]{figures/grid_sim_power.png}
\end{center}

\end{frame}


\section{Sampling \& Covariate Balance}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Sampling Methods for Experiments \emoji{scissors}}

\textbf{Why sampling matters:}
\begin{itemize}
    \item Limited resources (budget, time, traffic)
    \item \blue{Reduces variance} - Better precision in estimates
    \item \red{Increases credibility} - Shows randomization worked properly
\end{itemize}

\textbf{Two main approaches:}
\begin{itemize}
    \item \blue{Random Sampling} - Simple random selection
    \begin{itemize}
        \item Easy to implement, may result in imbalanced small segments
    \end{itemize}
    \item \red{Stratified Sampling} - Sample within dimensions
    \begin{itemize}
        \item Ensures representation across strata, better for heterogeneous populations
        \item Can use proportional or equal allocation
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Blocking / Stratified Sampling \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
from experiment_utils import balanced_random_assignment

# random allocation
treatment_unblock = balanced_random_assignment(
    df,
    variants=['treatment', 'control'],
    allocation_ratio=1/2,
    balance_covariates=['age', 'previous_purchases', 'days_since_signup'],
    seed=4321
)

Balance Check After Assignment

Comparison: control (n=1,500) vs treatment (n=1,500)

         covariate  n_control  n_treatment  mean_control  mean_treatment      smd balanced
               age       1500         1500     39.629457       39.578661 0.005167        y
previous_purchases       1500         1500      3.072000        2.958000 0.065706        y 
 days_since_signup       1500         1500    367.491830      345.775354 0.060344        y

Summary: 3/3 covariates balanced (|SMD| < 0.1)
Mean |SMD|: 0.0437
Max |SMD|: 0.0657

\end{minted}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Blocking / Stratified Sampling \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}

# stratified allocation
treatment_block = balanced_random_assignment(
    df,
    variants=['treatment', 'control'],
    allocation_ratio=1/2,
    stratification_covariates=['age', 'previous_purchases', 'days_since_signup'],
    seed=4321
)

Balance Check After Assignment

Comparison: treatment (n=1,500) vs control (n=1,500)
         covariate  n_treatment  n_control  mean_treatment  mean_control      smd balanced
               age         1500       1500       39.656748     39.551370 0.010718        y
previous_purchases         1500       1500        3.015333      3.014667 0.000384        y
 days_since_signup         1500       1500      358.103009    355.164175 0.008163        y

Summary: 3/3 covariates balanced (|SMD| < 0.1)
Mean |SMD|: 0.0064
Max |SMD|: 0.0107

\end{minted}
\end{frame}


\section{Analysis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Components \emoji{straight-ruler}}

\textbf{Key components:}
\begin{itemize}
    \item \blue{Primary metrics}
    \begin{itemize}
        \item Pre-specified metrics (e.g., revenue, conversion)
        \item Multiple tests?
    \end{itemize}
    \item \orange{Guardrail metrics}
    \begin{itemize}
        \item Safety checks (e.g., load time, error rates)
        \item Implementation metrics
    \end{itemize}
    \item \red{Sample Ratio Mismatch (SRM)}
    \begin{itemize}
        \item Does split match expectation?
        \item SRM signals implementation issues or bias
    \end{itemize}
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Distribution Assumptions: Don't Overthink It \emoji{brain}}

% \vspace{0.3cm}
\textbf{Why not useful:}
\begin{itemize}
    \item \blue{CLT (Central Limit Theorem):} Sample means $\approx$ normal for large n (>30-50), regardless of distribution
    \item Normality tests too sensitive; \orange{sampling distribution} matters, not population
\end{itemize}

\vspace{0.1cm}
\textbf{What to do:}
\begin{itemize}
    \item \textbf{Large samples:} Use standard t-tests / z-tests (rely on CLT)
    \item \textbf{Small samples/complex metrics:} Use \blue{bootstrapping}
    \item \textbf{Extreme outliers:} Robust statistics (better models!)
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Simple analysis \emoji{technologist} (1,500 users/group, 5 pp lift)}

\begin{minted}[fontsize=\tiny]{python}
from experiment_utils import ExperimentAnalyzer

analyzer_simple = ExperimentAnalyzer(
    df,
    treatment_col='treatment',
    outcomes='conversion',
    bootstrap=True,
    exp_sample_ratio=0.50,
    outcome_models={'conversion':['ols', 'logistic']},
    # pvalue_adjustment='sidak',
)
analyzer_simple.get_effects()
print(analyzer_simple.results.round(3)[['model_type', 'absolute_effect', 'relative_effect', 'srm_pvalue']])

  model_type  absolute_effect  relative_effect  srm_pvalue
0        ols            0.051            0.153         1.0
1   logistic            0.051            0.153         1.0
\end{minted}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Regression with Pre-Treatment Covariates \emoji{robot}}

\textbf{Regression adjustment} controls for pre-treatment covariates to partial out noise unrelated to treatment.

\textbf{Benefits (even with perfect randomization):}
\begin{itemize}
    \item \blue{Reduced variance} $\rightarrow$ narrower confidence intervals, higher precision
    \item \orange{Increased power} (especially when covariates correlate with outcome)
    \item \red{Corrects chance imbalances} in small samples ($n < 1000$/group)
    \item \textbf{Use when:} pre-treatment covariates available --- almost always worth it
\end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Regression Adjustment \emoji{robot}}

\begin{minted}[fontsize=\tiny]{python}
from experiment_utils import ExperimentAnalyzer

# no adjustment
analyzer_simple = ExperimentAnalyzer(
    df,
    treatment_col='treatment',
    outcomes='conversion',

)
analyzer_simple.get_effects()
print(analyzer_simple.results[['absolute_effect', 'pvalue', 'standard_error']])

   absolute_effect    pvalue  standard_error
0         0.051333  0.003411        0.017531

# covariate adjustment
analyzer_adjusted = ExperimentAnalyzer(
    df,
    treatment_col='treatment',
    outcomes='conversion',
    regression_covariates=['age', 'previous_purchases', 'days_since_signup']
)

analyzer_adjusted.get_effects()
print(analyzer_adjusted.results[['absolute_effect', 'pvalue', 'standard_error']])
   absolute_effect    pvalue  standard_error
0         0.050259  0.000039        0.012212

final_effect = analyzer_adjusted.results.loc[0, 'absolute_effect']

\end{minted}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Winner's Curse \emoji{skull} (Subsample of 500 users/group)}

\textbf{Problem:} Significant results often \red{overestimate} true effect

\vspace{0.2cm}
\textbf{Why?}
\begin{itemize}
    \item Selection bias: Only "winners" reported
    \item Small samples + low power $\rightarrow$ worse exaggeration (2-3x for power < 0.50)
\end{itemize}
\vspace{0.2cm}
$$\text{Exaggeration (Type M)} = \left|\frac{\hat{\tau}}{\tau}\right|$$
\vspace{-0.2cm}
$$\text{Relative Bias} = \frac{\hat{\tau}}{\tau}$$

\vspace{0.1cm}
{\small When an underpowered experiment reports a significant effect, divide the estimate by the exaggeration ratio to get a more realistic sense of the true effect size.}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[fragile]{Winner's Curse \emoji{skull}}

\begin{minted}[fontsize=\tiny]{python}

# we hunted for significant effect using a smaller sample (n=500/group)
print(analyzer_retro.results[['absolute_effect', 'pvalue', 'standard_error']])
   absolute_effect    pvalue  standard_error
0         0.080322  0.007778        0.030179

print(f'True effect: {final_effect:.4f}')
True effect: 0.0503

cols = ['power', 'type_s_error', 'type_m_error', 'relative_bias', 'trimmed_abs_effect']
print(analyzer_retro.calculate_retrodesign(true_effect=final_effect)[cols])
    power  type_s_error  type_m_error  relative_bias  trimmed_abs_effect
0  0.3996           0.0        1.5829         1.5829            0.050743

\end{minted}

\end{frame}

\section{Non-Compliance \& Confounding}

\begin{frame}{What If the Experiment Is Broken? \emoji{scream}}

\textbf{\red{Common problems:}}
\begin{itemize}
    \item \textbf{Implementation issues} - Bugs in assignment or logging
    \item \textbf{SRM (Sample Ratio Mismatch)} - Observed split differs from expected
    \item \textbf{Non-compliance} - Users don't receive assigned treatment
\end{itemize}

\vspace{0.2cm}
\textbf{\blue{What can we do?}}
\begin{itemize}
    \item \textbf{ITT (Intent to Treat)} - Preserves randomization, underestimates effect
    \item \textbf{Regression adjustment / IPW (Inverse Probability Weighting)} - Correct for imbalances
    \item \textbf{IV (Instrumental Variables)} - Assignment as instrument (ITT $\rightarrow$ LATE)
\end{itemize}

\end{frame}


\begin{frame}{Simulation: Non-Compliance + Confounding \emoji{technologist}}

\textbf{CS farming meeting} scenario:
\begin{itemize}
    \item Only a \% of treated users \blue{attend} (one-sided non-compliance)
    \item Attenders are more engaged, higher baseline revenue (\red{confounding})
\end{itemize}

\textbf{How to recover the true effect on bookings?}

\begin{description}
    \item[\blue{ITT}] As assigned $\rightarrow$ \red{underestimates} effect
    \item[\blue{Regression}] Covariate adjustment $\rightarrow$ reduces bias
    \item[\blue{IPW}] Inverse probability weighting $\rightarrow$ corrects imbalance
    \item[\blue{IV}] Assignment as instrument $\rightarrow$ \orange{LATE} $\equiv$ \blue{ATT} (one-sided non-compliance)
\end{description}

\end{frame}


\begin{frame}[fragile]{Naive Approach \emoji{teddy-bear}}

\textbf{Naive analysis:} Compare attenders vs non-attenders

\textbf{True effect} = 5 bookings

\begin{minted}[fontsize=\tiny]{python}

naive = ExperimentAnalyzer(
    cdf,
    treatment_col='attended',
    outcomes='bookings',
)

naive.get_effects()

naive.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']]

absolute_effect  abs_effect_lower  abs_effect_upper         pvalue
0         6.316905          5.921429          6.712381  3.823925e-215

\end{minted}
\end{frame}


\begin{frame}[fragile]{Intent-to-Treat (ITT) \emoji{balance-scale}}

\textbf{ITT:} Compare all treated vs control, regardless of attendance

\textbf{True effect} = 5 bookings

\begin{minted}[fontsize=\tiny]{python}

itt = ExperimentAnalyzer(
    cdf,
    treatment_col='assigned',
    outcomes='bookings',
)
itt.get_effects()

itt.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']]

absolute_effect  abs_effect_lower  abs_effect_upper        pvalue
0         2.743512          2.359612          3.127413  1.418363e-44

\end{minted}
\end{frame}


\begin{frame}[fragile]{Inverse Probability Weighting (IPW) \emoji{ninja}}

\textbf{IPW:} Adjust for compliance selection using covariates

\textbf{True effect} = 5 bookings
% \textbf{Note:} We are estimating ATT (effect on attenders) ~ effect on compliers

\begin{minted}[fontsize=\tiny]{python}

ipw = ExperimentAnalyzer(
    cdf,
    treatment_col='attended', outcomes='bookings',
    balance_covariates=['engagement', 'account_age_months', 'monthly_usage'],
    adjustment='balance',  balance_method='ps-logistic', estimand='ATT', overlap_plot=True)

ipw.get_effects()
ipw.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']]
   absolute_effect  abs_effect_lower  abs_effect_upper         pvalue
0         5.056775          4.652374          5.461177  1.212354e-132

\end{minted}
\end{frame}


\begin{frame}{Overlap Assumption \emoji{magnifying-glass-tilted-left}}

\textbf{\red{Why it matters:}}
\begin{itemize}
    \item IPW requires \blue{common support}: every unit needs a non-zero probability of either treatment
    \item Without it, weights become extreme $\rightarrow$ \red{high variance and bias}
\end{itemize}

\textbf{\blue{What to do:}}
\begin{itemize}
    \item \textbf{Diagnose:} Inspect propensity score distributions by group
    \item \textbf{Trim:} Drop or down-weight units outside common support
    \item \textbf{Target:} Shift estimand from ATE $\rightarrow$ ATT
\end{itemize}

\begin{center}
\includegraphics[width=0.35\textwidth]{figures/ipw-overlap.png}
\end{center}
\end{frame}


\begin{frame}[fragile]{Instrumental Variables (IV) \emoji{key}}

\textbf{IV:} Use assignment as instrument to isolate the effect on compliers (attenders)\\
\small{One-sided non-compliance: LATE $\equiv$ ATT, since there are no always-takers.}

\textbf{True effect} = 5 bookings

\begin{minted}[fontsize=\tiny]{python}
iv = ExperimentAnalyzer(
    cdf,
    treatment_col='attended',
    outcomes='bookings',
    regression_covariates=['engagement', 'account_age_months', 'monthly_usage'],
    instrument_col='assigned',
    adjustment='IV'
)

iv.get_effects()

print(iv.results[['absolute_effect', 'abs_effect_lower', 'abs_effect_upper', 'pvalue']])
   absolute_effect  abs_effect_lower  abs_effect_upper  pvalue
0         4.870595          4.381111           5.36008     0.0

\end{minted}

\end{frame}


\begin{frame}{Fixed-Effects Meta-Analysis \emoji{bar-chart}}

\textbf{Goal:} Pool effect estimates across multiple experiments into a single estimate

\textbf{\blue{Inverse-variance weighting:}}
\begin{itemize}
    \item Each experiment's estimate is weighted by $w_i = 1/\sigma_i^2$
    \item More precise experiments (smaller SE) get \blue{more weight}
\end{itemize}

\textbf{\orange{When to use it:}}
\begin{itemize}
    \item Multiple experiments testing the \blue{same intervention}
    \item Any single experiment underpowered on its own
    \item Experiments run across different \red{regions / segments}
\end{itemize}

\textbf{\red{Key assumption:}} All experiments share the same true effect (homogeneity)

\end{frame}


\begin{frame}[fragile]{Fixed-Effects Meta-Analysis \emoji{technologist}}

\textbf{Setup:} 5 experiments, \red{baselines correlated with allocation} $\rightarrow$ naive pooling is biased

\begin{minted}[fontsize=\tiny]{python}
# high-alloc -> low baseline; low-alloc -> high baseline
experiments = [
    {"name": "exp_1", "n": 3000, "alloc": 0.20, "baseline": 0.20},
    {"name": "exp_2", "n": 2000, "alloc": 0.30, "baseline": 0.25},
    {"name": "exp_3", "n": 1500, "alloc": 0.50, "baseline": 0.30},
    {"name": "exp_4", "n": 1000, "alloc": 0.70, "baseline": 0.40},
    {"name": "exp_5", "n": 800,  "alloc": 0.80, "baseline": 0.50},
]

analyzer = ExperimentAnalyzer(
    data=meta_df, treatment_col="treatment",
    outcomes=["conversion"], experiment_identifier="experiment",
)
analyzer.get_effects()

pooled = analyzer.combine_effects(grouping_cols=["outcome"])
print(pooled[["outcome", "experiments", "absolute_effect", "standard_error", "pvalue"]])
      outcome  experiments  absolute_effect  standard_error    pvalue
   conversion          5.0           0.0600          0.0114  1.54e-07

# True effect = 0.05
# Naive pooled  = 0.14  <-- 2.9x overestimate
# Meta-analysis = 0.06  <-- recovers true effect
\end{minted}

\end{frame}


\begin{frame}[fragile]{Fixed-Effects Meta-Analysis \emoji{technologist}}

\begin{minted}[fontsize=\tiny]{python}
fig = analyzer.plot_effects(meta_analysis=True)
\end{minted}

\begin{center}
\includegraphics[width=0.60\textwidth]{figures/effect-plot.png}
\end{center}
\end{frame}


\begin{frame}{Key takeaways \emoji{sparkles}}

\begin{itemize}
    \item Power analysis is crucial for experiment design; MDE should reflect business relevance, not statistical convenience
    \item Sampling methods (random vs stratified) can improve balance and precision
    \item Regression adjustment reduces variance and corrects for imbalances, even with perfect randomization
    \item Non-compliance and confounding can be addressed with ITT, IPW, and IV methods
    \item Meta-analysis allows pooling across multiple experiments
\end{itemize}

\end{frame}


\begin{frame}{Links}

\begin{itemize}
    \item \href{https://github.com/sdaza/slides/blob/main/presentations/experimentation/code/simulations.py}{Simulation code}
    \item \texttt{\href{https://github.com/sdaza/experiment-utils-pd}{experiment-utils-pd}}
\end{itemize}

\end{frame}

\end{document}

